{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "215a6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe3a5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.fixseed import fixseed\n",
    "from utils.parser_util import train_args\n",
    "from utils import dist_util\n",
    "from train.training_loop import TrainLoop\n",
    "from data_loaders.get_data import get_dataset_loader\n",
    "from utils.model_util import create_model_and_diffusion\n",
    "from train.train_platforms import ClearmlPlatform, TensorboardPlatform, NoPlatform  # required for the eval operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3501bc3",
   "metadata": {},
   "source": [
    "## Creating Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9da8c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pose_tensor = torch.tensor(np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Motion_feature.npy\"))\n",
    "keep_joints = [13,14,16,17,18,19,20,21]\n",
    "filtered = pose_tensor[:, :, keep_joints, :].reshape(-1,120,48)\n",
    "filtered = F.pad(filtered, (0, 31), mode=\"constant\", value=0)\n",
    "pose_tensor = filtered.reshape(-1,1,120,79).permute(0,3,1,2)\n",
    "\n",
    "audio_tensor = torch.tensor(np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Audio_feature.npy\"))\n",
    "#audio_tensor = F.pad(audio_tensor, (0, 74), mode=\"constant\", value=0)\n",
    "audio_tensor = audio_tensor.reshape(-1,1,120,64)\n",
    "\n",
    "beat_tensor = torch.tensor(np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Beat_feature.npy\"))\n",
    "#beat_tensor = F.pad(beat_tensor, (0, 33), mode=\"constant\", value=0)\n",
    "beat_tensor = beat_tensor.reshape(-1,1,120,15)\n",
    "\n",
    "full_tensor = torch.concatenate([audio_tensor,beat_tensor], axis=-1).permute(0,3,1,2)\n",
    "\n",
    "dataset = TensorDataset(full_tensor, pose_tensor)\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=5)\n",
    "#test_dataloader = DataLoader(dataset[15793:], batch_size=64, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a171e",
   "metadata": {},
   "source": [
    "## Model Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3e8b2c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import argparse\n",
    "\n",
    "def add_base_options(parser):\n",
    "    group = parser.add_argument_group('base')\n",
    "    group.add_argument(\"--cuda\", default=True, type=bool, help=\"Use cuda device, otherwise use CPU.\")\n",
    "    group.add_argument(\"--device\", default=0, type=int, help=\"Device id to use.\")\n",
    "    group.add_argument(\"--seed\", default=10, type=int, help=\"For fixing random seed.\")\n",
    "    group.add_argument(\"--batch_size\", default=64, type=int, help=\"Batch size during training.\")\n",
    "    group.add_argument(\"--short_db\", action='store_true', help=\"Load short babel for debug.\")\n",
    "    group.add_argument(\"--cropping_sampler\", action='store_true', help=\"Load short babel for debug.\")\n",
    "    \n",
    "def add_data_options(parser):\n",
    "    group = parser.add_argument_group('dataset')\n",
    "    group.add_argument(\"--dataset\", default='humanml', choices=['humanml', 'amass', 'babel'], type=str,\n",
    "                       help=\"Dataset name (choose from list).\")\n",
    "    group.add_argument(\"--data_dir\", default=\"\", type=str,\n",
    "                       help=\"If empty, will use defaults according to the specified dataset.\")\n",
    "    \n",
    "def add_model_options(parser):\n",
    "    group = parser.add_argument_group('model')\n",
    "    group.add_argument(\"--arch\", default='trans_enc',\n",
    "                       choices=['trans_enc', 'trans_dec', 'gru'], type=str,\n",
    "                       help=\"Architecture types as reported in the paper.\")\n",
    "    group.add_argument(\"--emb_trans_dec\", default=False, type=bool,\n",
    "                       help=\"For trans_dec architecture only, if true, will inject condition as a class token\"\n",
    "                            \" (in addition to cross-attention).\")\n",
    "    group.add_argument(\"--layers\", default=8, type=int,\n",
    "                       help=\"Number of layers.\")\n",
    "    group.add_argument(\"--latent_dim\", default=512, type=int,\n",
    "                       help=\"Transformer/GRU width.\")\n",
    "    group.add_argument(\"--cond_mask_prob\", default=.1, type=float,\n",
    "                       help=\"The probability of masking the condition during training.\"\n",
    "                            \" For classifier-free guidance learning.\")\n",
    "    group.add_argument(\"--lambda_rcxyz\", default=0.0, type=float, help=\"Joint positions loss.\")\n",
    "    group.add_argument(\"--lambda_vel\", default=0.0, type=float, help=\"Joint velocity loss.\")\n",
    "    group.add_argument(\"--lambda_fc\", default=0.0, type=float, help=\"Foot contact loss.\")\n",
    "    group.add_argument(\"--use_tta\", action='store_true', help=\"Time To Arrival position encoding\")  # FIXME REMOVE?\n",
    "    group.add_argument(\"--concat_trans_emb\", action='store_true', help=\"Concat transition emb, else append after linear\")  # FIXME REMOVE?\n",
    "    group.add_argument(\"--trans_emb\", action='store_true', help=\"Allow transition embedding\")  # FIXME REMOVE?\n",
    "    \n",
    "    group.add_argument(\"--context_len\", default=0, type=int, help=\"If larger than 0, will do prefix completion.\")\n",
    "    group.add_argument(\"--pred_len\", default=0, type=int, help=\"If context_len larger than 0, will do prefix completion. If pred_len will not be specified - will use the same length as context_len\")\n",
    "    \n",
    "def add_diffusion_options(parser):\n",
    "    group = parser.add_argument_group('diffusion')\n",
    "    group.add_argument(\"--noise_schedule\", default='cosine', choices=['linear', 'cosine'], type=str,\n",
    "                       help=\"Noise schedule type\")\n",
    "    group.add_argument(\"--diffusion_steps\", default=1000, type=int,\n",
    "                       help=\"Number of diffusion steps (denoted T in the paper)\")\n",
    "    group.add_argument(\"--sigma_small\", default=True, type=bool, help=\"Use smaller sigma values.\")\n",
    "    \n",
    "def add_training_options(parser):\n",
    "    group = parser.add_argument_group('training')\n",
    "    group.add_argument(\"--save_dir\", required=False, default=\"/workspace/priorMD/temporary_folder/test_our_chatgptData\", type=str,\n",
    "                       help=\"Path to save checkpoints and results.\")\n",
    "    group.add_argument(\"--overwrite\", action='store_true',\n",
    "                       help=\"If True, will enable to use an already existing save_dir.\")\n",
    "    group.add_argument(\"--train_platform_type\", default='NoPlatform', choices=['NoPlatform', 'ClearmlPlatform', 'TensorboardPlatform'], type=str,\n",
    "                       help=\"Choose platform to log results. NoPlatform means no logging.\")\n",
    "    group.add_argument(\"--lr\", default=1e-4, type=float, help=\"Learning rate.\")\n",
    "    group.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Optimizer weight decay.\")\n",
    "    group.add_argument(\"--lr_anneal_steps\", default=0, type=int, help=\"Number of learning rate anneal steps.\")\n",
    "    group.add_argument(\"--eval_batch_size\", default=32, type=int,\n",
    "                       help=\"Batch size during evaluation loop. Do not change this unless you know what you are doing. \"\n",
    "                            \"T2m precision calculation is based on fixed batch size 32.\")\n",
    "    group.add_argument(\"--eval_split\", default='test', choices=['val', 'test'], type=str,\n",
    "                       help=\"Which split to evaluate on during training.\")\n",
    "    group.add_argument(\"--eval_during_training\", action='store_true',\n",
    "                       help=\"If True, will run evaluation during training.\")\n",
    "    group.add_argument(\"--eval_rep_times\", default=3, type=int,\n",
    "                       help=\"Number of repetitions for evaluation loop during training.\")\n",
    "    group.add_argument(\"--eval_num_samples\", default=1_000, type=int,\n",
    "                       help=\"If -1, will use all samples in the specified split.\")\n",
    "    group.add_argument(\"--log_interval\", default=1_000, type=int,\n",
    "                       help=\"Log losses each N steps\")\n",
    "    group.add_argument(\"--save_interval\", default=6_000, type=int,\n",
    "                       help=\"Save checkpoints and run evaluation each N steps\")\n",
    "    group.add_argument(\"--num_steps\", default=300_000, type=int,\n",
    "                       help=\"Training will stop after the specified number of steps.\")\n",
    "    group.add_argument(\"--num_frames\", default=120, type=int,\n",
    "                       help=\"Limit for the maximal number of frames. In HumanML3D and KIT this field is ignored.\")\n",
    "    group.add_argument(\"--resume_checkpoint\", default=\"\", type=str,\n",
    "                       help=\"If not empty, will start from the specified checkpoint (path to model###.pt file).\")\n",
    "    \n",
    "def add_frame_sampler_options(parser):\n",
    "    group = parser.add_argument_group('framesampler')\n",
    "    group.add_argument(\"--min_seq_len\", default=45, type=int,\n",
    "                       help=\"babel dataset FrameSampler minimum length\")\n",
    "    group.add_argument(\"--max_seq_len\", default=250, type=int,\n",
    "                       help=\"babel dataset FrameSampler maximum length\")\n",
    "\n",
    "parser = ArgumentParser()\n",
    "add_base_options(parser)\n",
    "add_data_options(parser)\n",
    "add_model_options(parser)\n",
    "add_diffusion_options(parser)\n",
    "add_training_options(parser)\n",
    "add_frame_sampler_options(parser)\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "#args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a68bc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import clip\n",
    "import os\n",
    "#from priorMD.model.rotation2xyz import Rotation2xyz\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, njoints, nfeats, num_actions, translation, pose_rep, glob, glob_rot,\n",
    "                 latent_dim=512, ff_size=1024, num_layers=8, num_heads=4, dropout=0.1,\n",
    "                 ablation=None, activation=\"gelu\", legacy=False, data_rep='rot6d', dataset='amass', clip_dim=512,\n",
    "                 arch='trans_enc', emb_trans_dec=False, clip_version=None, **kargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.legacy = legacy\n",
    "        #self.modeltype = modeltype\n",
    "        self.njoints = njoints\n",
    "        self.nfeats = nfeats\n",
    "        self.num_actions = num_actions\n",
    "        self.data_rep = data_rep\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.pose_rep = pose_rep\n",
    "        self.glob = glob\n",
    "        self.glob_rot = glob_rot\n",
    "        self.translation = translation\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.ff_size = ff_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.ablation = ablation\n",
    "        self.activation = activation\n",
    "        self.clip_dim = clip_dim\n",
    "        self.action_emb = kargs.get('action_emb', None)\n",
    "\n",
    "        self.input_feats = self.njoints * self.nfeats\n",
    "\n",
    "        self.normalize_output = kargs.get('normalize_encoder_output', False)\n",
    "\n",
    "        self.cond_mode = 'text'# kargs.get('cond_mode', 'no_cond')\n",
    "        self.cond_mask_prob = kargs.get('cond_mask_prob', 0.)\n",
    "        self.arch = arch\n",
    "        self.gru_emb_dim = self.latent_dim if self.arch == 'gru' else 0\n",
    "        self.Cam_input_process = CamInputProcess(self.data_rep, self.input_feats+self.gru_emb_dim, self.latent_dim)\n",
    "        self.input_process = InputProcess(self.data_rep, self.input_feats+self.gru_emb_dim, self.latent_dim)\n",
    "        \n",
    "        self.sequence_pos_encoder = PositionalEncoding(self.latent_dim, self.dropout)\n",
    "        self.cond_pos_encoder = PositionalEncoding(self.latent_dim, self.dropout)\n",
    "        self.cam_sequence_pos_encoder = CamPositionalEncoding(self.latent_dim, self.dropout)\n",
    "        self.emb_trans_dec = emb_trans_dec\n",
    "        \n",
    "        self.Camera_input_process = nn.Linear(6, self.latent_dim)\n",
    "        self.Camera_output_process = nn.Linear(self.latent_dim, 6)\n",
    "        \n",
    "        self.encode_img = nn.Linear(768, 512)\n",
    "        self.encode_output = nn.Linear(1536, 512)\n",
    "        self.cond_proj = nn.Linear(512, 512)\n",
    "        self.ada_mlp = nn.Linear(1024, 1024)\n",
    "        self.ln = nn.LayerNorm(latent_dim)\n",
    "        self.mlp = nn.Linear(512, 512)\n",
    "        \n",
    "        self.camera_person = CameraPersonBlock(arch=\"trans_enc\",\n",
    "                                             fn_type='in_both_out_cur',\n",
    "                                             num_layers=2,\n",
    "                                             latent_dim=self.latent_dim,\n",
    "                                             input_feats=self.input_feats,\n",
    "                                             predict_6dof=True)\n",
    "\n",
    "        if self.arch == 'trans_enc':\n",
    "            #assert 0 < self.args.multi_backbone_split <= self.num_layers\n",
    "            print(f'CUTTING BACKBONE AT LAYER 8')\n",
    "            seqTransEncoderLayer = nn.TransformerEncoderLayer(d_model=self.latent_dim,\n",
    "                                                              nhead=self.num_heads,\n",
    "                                                              dim_feedforward=self.ff_size,\n",
    "                                                              dropout=self.dropout,\n",
    "                                                              activation=self.activation)\n",
    "            \n",
    "            CamTransEncoderLayer = nn.TransformerEncoderLayer(d_model=self.latent_dim,\n",
    "                                                              nhead=self.num_heads,\n",
    "                                                              dim_feedforward=self.ff_size,\n",
    "                                                              dropout=self.dropout,\n",
    "                                                              activation=self.activation)\n",
    "\n",
    "            self.CamTransEncoder = nn.TransformerEncoder(CamTransEncoderLayer,\n",
    "                                                         num_layers=self.num_layers)\n",
    "        \n",
    "            self.seqTransEncoder_start = nn.TransformerEncoder(seqTransEncoderLayer,\n",
    "                                                               num_layers=8)\n",
    "            self.seqTransEncoder_end = nn.TransformerEncoder(seqTransEncoderLayer,\n",
    "                                                             num_layers=self.num_layers - 8)\n",
    "\n",
    "        self.embed_timestep = TimestepEmbedder(self.latent_dim, self.sequence_pos_encoder)\n",
    "\n",
    "        if self.cond_mode != 'no_cond':\n",
    "            if 'text' in self.cond_mode:\n",
    "                self.embed_text = nn.Linear(self.clip_dim, self.latent_dim)\n",
    "                print('EMBED TEXT')\n",
    "                print('Loading CLIP...')\n",
    "               # self.clip_version = clip_version\n",
    "               # self.clip_model = self.load_and_freeze_clip(clip_version)\n",
    "            if 'action' in self.cond_mode:\n",
    "                self.embed_action = EmbedAction(self.num_actions, self.latent_dim)\n",
    "                print('EMBED ACTION')\n",
    "\n",
    "        self.Cam_output_process = CamOutputProcess(self.data_rep, self.input_feats, self.latent_dim, self.njoints,\n",
    "                                            self.nfeats)\n",
    "\n",
    "        #self.rot2xyz = Rotation2xyz(device='cpu', dataset=self.dataset)\n",
    "        self.series_model = TimeSeriesTransformer(input_dim=150, embed_dim=latent_dim, num_heads=8, num_layers=4)\n",
    "        \n",
    "        self.motion_cross_attn = nn.MultiheadAttention(latent_dim, num_heads=4, batch_first=True)\n",
    "        self.cross_attn_x_to_y = nn.MultiheadAttention(latent_dim, num_heads=4, batch_first=True)\n",
    "        self.motion_norm = nn.LayerNorm(latent_dim)\n",
    "\n",
    "    def parameters_wo_clip(self):\n",
    "        return [p for name, p in self.named_parameters() if not name.startswith('clip_model.')]\n",
    "\n",
    "    def load_and_freeze_clip(self, clip_version):\n",
    "        clip_model, clip_preprocess = clip.load(clip_version, device='cpu',\n",
    "                                                jit=False)  # Must set jit=False for training\n",
    "        clip.model.convert_weights(\n",
    "            clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "        # Freeze CLIP weights\n",
    "        clip_model.eval()\n",
    "        for p in clip_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        return clip_model\n",
    "\n",
    "    def mask_cond(self, cond, force_mask=False):\n",
    "        bs, d = cond.shape\n",
    "        if force_mask:\n",
    "            return torch.zeros_like(cond)\n",
    "        elif self.training and self.cond_mask_prob > 0.:\n",
    "            mask = torch.bernoulli(torch.ones(bs, device=cond.device) * self.cond_mask_prob).view(bs, 1)  # 1-> use null_cond, 0-> use real cond\n",
    "            return cond * (1. - mask)\n",
    "        else:\n",
    "            return cond\n",
    "\n",
    "    def encode_text(self, raw_text):\n",
    "        # raw_text - list (batch_size length) of strings with input text prompts\n",
    "        device = next(self.parameters()).device\n",
    "        max_text_len = 20 if self.dataset in ['humanml', 'kit'] else None  # Specific hardcoding for humanml dataset\n",
    "        if max_text_len is not None:\n",
    "            default_context_length = 77\n",
    "            context_length = max_text_len + 2 # start_token + 20 + end_token\n",
    "            assert context_length < default_context_length\n",
    "            texts = clip.tokenize(raw_text, context_length=context_length, truncate=True).to(device) # [bs, context_length] # if n_tokens > context_length -> will truncate\n",
    "            # print('texts', texts.shape)\n",
    "            zero_pad = torch.zeros([texts.shape[0], default_context_length-context_length], dtype=texts.dtype, device=texts.device)\n",
    "            texts = torch.cat([texts, zero_pad], dim=1)\n",
    "            # print('texts after pad', texts.shape, texts)\n",
    "        else:\n",
    "            texts = clip.tokenize(raw_text, truncate=True).to(device) # [bs, context_length] # if n_tokens > 77 -> will truncate\n",
    "        return self.clip_model.encode_text(texts).float()\n",
    "\n",
    "    def forward(self, x, timesteps, y=None):\n",
    "        #print(x.shape)\n",
    "        \"\"\"\n",
    "        x: [batch_size, njoints, nfeats, max_frames], denoted x_t in the paper\n",
    "        timesteps: [batch_size] (int)\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(y.shape)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        \n",
    "        bs, njoints, nfeats, nframes = x.shape\n",
    "        emb = self.embed_timestep(timesteps)  # [1, bs, d\n",
    "        \n",
    "        #seq1 = y[:,:,0,:]\n",
    "        #seq2 = y[:,:,1,:]\n",
    "        \n",
    "        x_cur = self.input_process(y) #[seqlen, bs, d]\n",
    "       # x_other = self.input_process(seq2.unsqueeze(2))\n",
    "        x_camera = self.Cam_input_process(x)\n",
    "        #print(x_camera.shape)\n",
    "\n",
    "        #print(emb.shape, x.shape, x_other.shape, x_camera.shape)\n",
    "\n",
    "        low_x = x_cur\n",
    "        low_x_camera = x_camera\n",
    "        #print(emb.shape,x.shape)\n",
    "\n",
    "        # adding the timestep embed\n",
    "        xseq = torch.cat((emb, x_cur), axis=0)  # [seqlen+1, bs, d]\n",
    "        xseq = self.sequence_pos_encoder(xseq)  # [seqlen+1, bs, d]\n",
    "        #x_other = torch.cat((emb, x_other), axis=0)\n",
    "        #x_other = self.sequence_pos_encoder(x_other)\n",
    "        x_camera = torch.cat((emb, x_camera), axis=0)\n",
    "        x_camera = self.cam_sequence_pos_encoder(x_camera)\n",
    "\n",
    "\n",
    "        mid = self.seqTransEncoder_start(xseq)[1:]\n",
    "        #mid_other = self.seqTransEncoder_start(x_other)[1:]\n",
    "        mid_Camera = self.CamTransEncoder(x_camera)[1:]\n",
    "\n",
    "\n",
    "        delta_camera_1, delta_x_1 = self.camera_person(low_cur=low_x_camera, \n",
    "                                                        low_other=low_x, \n",
    "                                                       cur=mid_Camera, \n",
    "                                                        other=mid)\n",
    "\n",
    "        #delta_camera_2, delta_x_other_1 = self.camera_person(low_cur=low_x_camera, \n",
    "                                                           # low_other=low_x_other, \n",
    "                                                          #  cur=mid_Camera, \n",
    "                                                           # other=mid_other)\n",
    "\n",
    "        mid_Camera += delta_camera_1           \n",
    "\n",
    "        output_camera = mid_Camera\n",
    "            #print(\"222222222\")\n",
    "\n",
    "        #print(output_x.shape, output_other.shape, output_camera.shape)\n",
    "\n",
    "        output_camera = self.Cam_output_process(output_camera)\n",
    "        \n",
    "        return output_camera\n",
    "\n",
    "\n",
    "    #def _apply(self, fn):\n",
    "        #super()._apply(fn)\n",
    "        #self.rot2xyz.smpl_model._apply(fn)\n",
    "\n",
    "\n",
    "    def train(self, *args, **kwargs):\n",
    "        super().train(*args, **kwargs)\n",
    "        #self.rot2xyz.smpl_model.train(*args, **kwargs)\n",
    "\n",
    "        \n",
    "# ---- Learnable Positional Encoding ----\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embed[:, :x.size(1)]\n",
    "\n",
    "# ---- Transformer Encoder for Time Series ----\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=512, num_heads=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.pos_encoding = LearnablePositionalEncoding(embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=1024, dropout=0.1, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.norm(x)\n",
    "    \n",
    "    \n",
    "class CamPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(CamPositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # not used in the final model\n",
    "        x = x + self.pe[:x.shape[0], :]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # not used in the final model\n",
    "        x = x + self.pe[:x.shape[0], :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    def __init__(self, latent_dim, sequence_pos_encoder):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_pos_encoder = sequence_pos_encoder\n",
    "\n",
    "        time_embed_dim = self.latent_dim\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        return self.time_embed(self.sequence_pos_encoder.pe[timesteps]).permute(1, 0, 2)\n",
    "    \n",
    "class CameraPersonBlock(nn.Module):\n",
    "    def __init__(self, arch, fn_type, num_layers, latent_dim, input_feats, predict_6dof):\n",
    "        super().__init__()\n",
    "        self.arch = arch\n",
    "        self.fn_type = fn_type\n",
    "        self.predict_6dof = predict_6dof\n",
    "        self.num_layers = num_layers\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = 4\n",
    "        self.ff_size = 1024\n",
    "        self.dropout = 0.1\n",
    "        self.activation = 'gelu'\n",
    "        self.input_feats = input_feats\n",
    "        if self.predict_6dof:\n",
    "            self.canon_agg = nn.Linear(9*2, self.latent_dim)\n",
    "            # self.canon_agg = nn.Linear(9*2, self.latent_dim)\n",
    "            # self.canon_agg = nn.Linear(self.input_feats*2, self.latent_dim)\n",
    "            self.canon_out = nn.Linear(self.latent_dim, 9)\n",
    "            # self.canon_out = nn.Linear(self.latent_dim, self.input_feats)\n",
    "        if 'in_both' in self.fn_type:\n",
    "            self.aggregation = nn.Linear(self.latent_dim*2, self.latent_dim)\n",
    "        if self.arch == 'trans_enc':\n",
    "            seqTransEncoderLayer = nn.TransformerEncoderLayer(d_model=self.latent_dim,\n",
    "                                                              nhead=self.num_heads,\n",
    "                                                              dim_feedforward=self.ff_size,\n",
    "                                                              dropout=self.dropout,\n",
    "                                                              activation=self.activation)\n",
    "            self.model = nn.TransformerEncoder(seqTransEncoderLayer,\n",
    "                                               num_layers=self.num_layers)\n",
    "        self.cross_attention = CrossAttention(embed_size=512, heads=4) \n",
    "        self.con_global = nn.Linear(self.latent_dim, 9)\n",
    "        self.avg_pooling = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.max_pooling = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "    def forward(self, low_cur=None, low_other=None, other=None, cur=None):     \n",
    "        low_x, low_x_other = low_cur + cur, low_other + other\n",
    "        # 交换concat顺序计算向量\n",
    "        x = self.aggregation(torch.concatenate((low_x, low_x_other), dim=-1))\n",
    "        x_other = self.aggregation(torch.concatenate((low_x_other, low_x), dim=-1))\n",
    "        # print(\"COMDMD中的x维度:\", x.shape) torch.Size([120, 64, 512])\n",
    "        x_out = self.model(x)# torch.Size([120, 64, 512])\n",
    "        x_other_out= self.model(x_other)# torch.Size([120, 64, 512])\n",
    "        # print(\"文本向量的emb:\", text_emb.shape) torch.Size([1, 64, 512])\n",
    "        # cross attention \n",
    "        #xA_cross, _ = self.cross_attn_B2A(x_out, x_other_out, x_other_out)\n",
    "        #xB_cross, _ = self.cross_attn_A2B(x_other_out, x_out, x_out)\n",
    "\n",
    "        return x_out, x_other_out\n",
    "\n",
    "\n",
    "class CamInputProcess(nn.Module):\n",
    "    def __init__(self, data_rep, input_feats, latent_dim):\n",
    "        super().__init__()\n",
    "        self.data_rep = data_rep\n",
    "        self.input_feats = input_feats\n",
    "        self.latent_dim = latent_dim\n",
    "        self.poseEmbedding = nn.Linear(self.input_feats, self.latent_dim)\n",
    "        if self.data_rep == 'rot_vel':\n",
    "            self.velEmbedding = nn.Linear(self.input_feats, self.latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, njoints, nfeats, nframes = x.shape\n",
    "        x = x.permute((3, 0, 1, 2)).reshape(nframes, bs, njoints*nfeats)\n",
    "\n",
    "        if self.data_rep in ['rot6d', 'xyz', 'hml_vec']:\n",
    "            x = self.poseEmbedding(x)  # [seqlen, bs, d]\n",
    "            return x\n",
    "        elif self.data_rep == 'rot_vel':\n",
    "            first_pose = x[[0]]  # [1, bs, 150]\n",
    "            first_pose = self.poseEmbedding(first_pose)  # [1, bs, d]\n",
    "            vel = x[1:]  # [seqlen-1, bs, 150]\n",
    "            vel = self.velEmbedding(vel)  # [seqlen-1, bs, d]\n",
    "            return torch.cat((first_pose, vel), axis=0)  # [seqlen, bs, d]\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "\n",
    "class CamOutputProcess(nn.Module):\n",
    "    def __init__(self, data_rep, input_feats, latent_dim, njoints, nfeats):\n",
    "        super().__init__()\n",
    "        self.data_rep = data_rep\n",
    "        self.input_feats = input_feats\n",
    "        self.latent_dim = latent_dim\n",
    "        self.njoints = njoints\n",
    "        self.nfeats = nfeats\n",
    "        self.poseFinal = nn.Linear(self.latent_dim, self.input_feats)\n",
    "        if self.data_rep == 'rot_vel':\n",
    "            self.velFinal = nn.Linear(self.latent_dim, self.input_feats)\n",
    "\n",
    "    def forward(self, output):\n",
    "        nframes, bs, d = output.shape\n",
    "        if self.data_rep in ['rot6d', 'xyz', 'hml_vec']:\n",
    "            output = self.poseFinal(output)  # [seqlen, bs, 150]\n",
    "        elif self.data_rep == 'rot_vel':\n",
    "            first_pose = output[[0]]  # [1, bs, d]\n",
    "            first_pose = self.poseFinal(first_pose)  # [1, bs, 150]\n",
    "            vel = output[1:]  # [seqlen-1, bs, d]\n",
    "            vel = self.velFinal(vel)  # [seqlen-1, bs, 150]\n",
    "            output = torch.cat((first_pose, vel), axis=0)  # [seqlen, bs, 150]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        output = output.reshape(nframes, bs, self.njoints, self.nfeats)\n",
    "        output = output.permute(1, 2, 3, 0)  # [bs, njoints, nfeats, nframes]\n",
    "        return output\n",
    "    \n",
    "class InputProcess(nn.Module):\n",
    "    def __init__(self, data_rep, input_feats, latent_dim):\n",
    "        super().__init__()\n",
    "        self.data_rep = data_rep\n",
    "        self.input_feats = input_feats\n",
    "        self.latent_dim = latent_dim\n",
    "        self.poseEmbedding = nn.Linear(self.input_feats, self.latent_dim)\n",
    "        if self.data_rep == 'rot_vel':\n",
    "            self.velEmbedding = nn.Linear(self.input_feats, self.latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, njoints, nfeats, nframes = x.shape\n",
    "        x = x.permute((3, 0, 1, 2)).reshape(nframes, bs, njoints*nfeats)\n",
    "\n",
    "        if self.data_rep in ['rot6d', 'xyz', 'hml_vec']:\n",
    "            x = self.poseEmbedding(x)  # [seqlen, bs, d]\n",
    "            return x\n",
    "        elif self.data_rep == 'rot_vel':\n",
    "            first_pose = x[[0]]  # [1, bs, 150]\n",
    "            first_pose = self.poseEmbedding(first_pose)  # [1, bs, d]\n",
    "            vel = x[1:]  # [seqlen-1, bs, 150]\n",
    "            vel = self.velEmbedding(vel)  # [seqlen-1, bs, d]\n",
    "            return torch.cat((first_pose, vel), axis=0)  # [seqlen, bs, d]\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "\n",
    "class OutputProcess(nn.Module):\n",
    "    def __init__(self, data_rep, input_feats, latent_dim, njoints, nfeats):\n",
    "        super().__init__()\n",
    "        self.data_rep = data_rep\n",
    "        self.input_feats = input_feats\n",
    "        self.latent_dim = latent_dim\n",
    "        self.njoints = njoints\n",
    "        self.nfeats = nfeats\n",
    "        self.poseFinal = nn.Linear(self.latent_dim, self.input_feats)\n",
    "        if self.data_rep == 'rot_vel':\n",
    "            self.velFinal = nn.Linear(self.latent_dim, self.input_feats)\n",
    "\n",
    "    def forward(self, output):\n",
    "        nframes, bs, d = output.shape\n",
    "        if self.data_rep in ['rot6d', 'xyz', 'hml_vec']:\n",
    "            output = self.poseFinal(output)  # [seqlen, bs, 150]\n",
    "        elif self.data_rep == 'rot_vel':\n",
    "            first_pose = output[[0]]  # [1, bs, d]\n",
    "            first_pose = self.poseFinal(first_pose)  # [1, bs, 150]\n",
    "            vel = output[1:]  # [seqlen-1, bs, d]\n",
    "            vel = self.velFinal(vel)  # [seqlen-1, bs, 150]\n",
    "            output = torch.cat((first_pose, vel), axis=0)  # [seqlen, bs, 150]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        output = output.reshape(nframes, bs, self.njoints, self.nfeats)\n",
    "        output = output.permute(1, 2, 3, 0)  # [bs, njoints, nfeats, nframes]\n",
    "        return output\n",
    "\n",
    "\n",
    "class EmbedAction(nn.Module):\n",
    "    def __init__(self, num_actions, latent_dim):\n",
    "        super().__init__()\n",
    "        self.action_embedding = nn.Parameter(torch.randn(num_actions, latent_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        idx = input[:, 0].to(torch.long)  # an index array must be long\n",
    "        output = self.action_embedding[idx]\n",
    "        return output\n",
    "    \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        ori_v = values\n",
    "        values, keys, query = values.permute(1,0,2),keys.permute(1,0,2), query.permute(1,0,2)\n",
    "        query = torch.repeat_interleave(query, repeats=values.shape[1],dim=1)\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Attention mechanism\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        out =  out.permute(1,0,2) + ori_v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd080a",
   "metadata": {},
   "source": [
    "## Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64228ce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUTTING BACKBONE AT LAYER 8\n",
      "EMBED TEXT\n",
      "Loading CLIP...\n"
     ]
    }
   ],
   "source": [
    "from diffusion.respace import SpacedDiffusion, space_timesteps\n",
    "from utils.model_util import create_gaussian_diffusion\n",
    "from diffusion.gaussian_diffusion import (\n",
    "    GaussianDiffusion,\n",
    "    get_named_beta_schedule,\n",
    "    create_named_schedule_sampler,\n",
    "    ModelMeanType,\n",
    "    ModelVarType,\n",
    "    LossType\n",
    ")\n",
    "\n",
    "model = DiffusionModel(njoints=79, nfeats=1, num_actions=1, translation=True, pose_rep=\"rot6d\", glob=True, glob_rot=True).to(\"cuda\")\n",
    "\n",
    "sampler_name = 'uniform'\n",
    "beta_scheduler = 'linear'\n",
    "betas = get_named_beta_schedule(beta_scheduler, args.diffusion_steps)\n",
    "diffusion = GaussianDiffusion(\n",
    "            betas=betas,\n",
    "            model_mean_type=ModelMeanType.EPSILON,\n",
    "            model_var_type=ModelVarType.FIXED_SMALL,\n",
    "            loss_type=LossType.MSE\n",
    "        )\n",
    "sampler = create_named_schedule_sampler(sampler_name, diffusion)\n",
    "\n",
    "n_epoch = 10000\n",
    "batch_size = 32\n",
    "n_T = 1000 # 500\n",
    "device = \"cuda\"\n",
    "n_feature = 5\n",
    "n_textemb = 512\n",
    "lrate = 0.0001\n",
    "save_model = True\n",
    "save_dir = './weight/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "optim = torch.optim.Adam(model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5118cde",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2395e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    print(f'epoch {ep}')\n",
    "    model.train()\n",
    "\n",
    "    # linear lrate decay\n",
    "    optim.param_groups[0]['lr'] = 0.0001*(1-ep/n_epoch)\n",
    "\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    loss_ema = None\n",
    "    for x in pbar:\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        motion = x[1].to(device).to(torch.float32)\n",
    "        audio = x[0].to(device).to(torch.float32)\n",
    "        \n",
    "        img_dic = {\"y\":audio}\n",
    "        \n",
    "        c = torch.zeros([64,512])\n",
    "        t, _ = sampler.sample(motion.shape[0], device)\n",
    "        \n",
    "\n",
    "        loss = diffusion.training_losses(\n",
    "                                            model=model,\n",
    "                                            x_start=motion,\n",
    "                                            t=t,\n",
    "                                            model_kwargs=img_dic\n",
    "                                            #model_kwargs={}\n",
    "                                        )[\"mse\"].to(torch.float32)\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        if loss_ema is None:\n",
    "            loss_ema = loss.item()\n",
    "        else:\n",
    "            loss_ema = 0.95*loss_ema+0.05*loss.item()\n",
    "        pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "        optim.step()\n",
    "\n",
    "    torch.save(model.state_dict(), save_dir + f\"conductor_latest.pth\")\n",
    "    if save_model and ep % 100 == 0:\n",
    "        torch.save(model.state_dict(), save_dir + f\"conductor_model_{ep}.pth\")\n",
    "        print('saved model at ' + save_dir + f\"conductor_model_{ep}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5e2403f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 138, 1, 120])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb = audio_tensor[::1000].to(\"cuda\").to(torch.float32)\n",
    "test_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47897060",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_list=[]\n",
    "A_Mean = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Audio_Mean.npy\")\n",
    "A_Std = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Audio_Std.npy\")\n",
    "for audio_file in os.listdir(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Mel_Clips/liu/\"):\n",
    "    if(audio_file.split(\"_\")[1]==\"1.npy\"):\n",
    "        test_audio_list.append((np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Mel_Clips/liu/\"+audio_file)-A_Mean)/(A_Std+0.000000001))\n",
    "        #print(audio_file)\n",
    "test_audio_tensor = torch.tensor(np.array(test_audio_list))\n",
    "#test_audio_tensor = F.pad(test_audio_tensor, (0, 74), mode=\"constant\", value=0)\n",
    "test_audio_tensor = test_audio_tensor.reshape(-1,1,120,64).permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "964bd9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_beat_list=[]\n",
    "B_Mean = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Beat_Mean.npy\")\n",
    "B_Std = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Beat_Std.npy\")\n",
    "for audio_file in os.listdir(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Beat_Clips/liu/\"):\n",
    "    if(audio_file.split(\"_\")[1]==\"1.npy\"):\n",
    "        test_beat_list.append((np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Beat_Clips/liu/\"+audio_file)-B_Mean)/(B_Std+0.000000001))\n",
    "        #print(audio_file)\n",
    "test_beat_tensor = torch.tensor(np.array(test_beat_list))\n",
    "#test_beat_tensor = F.pad(test_beat_tensor, (0, 33), mode=\"constant\", value=0)\n",
    "test_beat_tensor = test_beat_tensor.reshape(-1,1,120,15).permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42d53d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_tensor = torch.concatenate([test_audio_tensor,test_beat_tensor], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30f0b69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727073826d404c52bb2865fef08d5453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"weight/conductor_beat_mel_model_500.pth\", map_location=torch.device('cuda')))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    test_emb = test_full_tensor[20*i:20*i+20].to(\"cuda\").to(torch.float32)\n",
    "    test_dic = {\"y\":test_emb}\n",
    "    \n",
    "    test_array = diffusion.p_sample_loop(\n",
    "                model,\n",
    "                (20, 79, 1,120),\n",
    "                clip_denoised=False,\n",
    "                progress=True,\n",
    "                #model_kwargs={})\n",
    "                model_kwargs=test_dic)\n",
    "    break\n",
    "    \n",
    "    #np.save(\"Evaluation/beat_\"+str(i)+\".npy\",np.array(test_array.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08a1cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "Mean = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Motion_Mean.npy\")\n",
    "Std = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Motion_Std.npy\")\n",
    "\n",
    "\n",
    "#Mean = np.load(\"../Camera/Mix_Mean.npy\")\n",
    "#Std = np.load(\"../Camera/Mix_Std.npy\")\n",
    "#D_mean = np.load(\"../Camera/Mix_D_Mean.npy\")\n",
    "#D_std = np.load(\"../Camera/Mix_D_Std.npy\")\n",
    "\n",
    "test_data = test_array.cpu().detach().numpy().transpose(2,0,3,1)\n",
    "motion1 = test_data[0][:,:,:48].reshape(-1,120,8,6)\n",
    "\n",
    "m = motion1[4]\n",
    "\n",
    "arr_full = np.zeros((120, 23, 6))\n",
    "\n",
    "target_idx = [13, 14, 16, 17, 18, 19, 20, 21]   # 对应 13,14,16,17,18,19,20,21\n",
    "\n",
    "# 1) 把 (120,8,6) 填进去\n",
    "for i, t in enumerate(target_idx):\n",
    "    arr_full[:, t, :] = m[:, i, :]\n",
    "    \n",
    "arr_full = arr_full*(Std)+Mean\n",
    "\n",
    "# 2) 其他位置用 (23,6) 的值补充\n",
    "for j in range(23):\n",
    "    if j not in target_idx:\n",
    "        arr_full[:, j, :] = Mean[j]\n",
    "\n",
    "d = np.array([1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p0.npy\",arr_full)\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p1.npy\",arr_full)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p0.npy\",d)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p1.npy\",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb57941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
