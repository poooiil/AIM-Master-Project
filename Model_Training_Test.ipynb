{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "215a6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3322c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe3a5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.fixseed import fixseed\n",
    "from utils.parser_util import train_args\n",
    "from utils import dist_util\n",
    "from train.training_loop import TrainLoop\n",
    "from data_loaders.get_data import get_dataset_loader\n",
    "from utils.model_util import create_model_and_diffusion\n",
    "from train.train_platforms import ClearmlPlatform, TensorboardPlatform, NoPlatform  # required for the eval operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3501bc3",
   "metadata": {},
   "source": [
    "## Creating Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9da8c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "pose_tensor = torch.tensor(np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Motion_feature.npy\"))\n",
    "keep_joints = [13,14,16,17,18,19,20,21]\n",
    "filtered = pose_tensor[:, :, keep_joints, :].reshape(-1,120,48)\n",
    "filtered = F.pad(filtered, (0, 31), mode=\"constant\", value=0)\n",
    "pose_tensor = filtered.reshape(-1,1,120,79).permute(0,3,1,2)\n",
    "\n",
    "audio_tensor = torch.tensor(np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Audio_feature.npy\"))\n",
    "#audio_tensor = F.pad(audio_tensor, (0, 74), mode=\"constant\", value=0)\n",
    "audio_tensor = audio_tensor.reshape(-1,1,120,64)\n",
    "\n",
    "beat_tensor = torch.tensor(np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Beat_feature.npy\"))\n",
    "#beat_tensor = F.pad(beat_tensor, (0, 33), mode=\"constant\", value=0)\n",
    "beat_tensor = beat_tensor.reshape(-1,1,120,15)\n",
    "\n",
    "full_tensor = torch.concatenate([audio_tensor,beat_tensor], axis=-1).permute(0,3,1,2)\n",
    "\n",
    "dataset = TensorDataset(full_tensor, pose_tensor)\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=5)\n",
    "#test_dataloader = DataLoader(dataset[15793:], batch_size=64, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a171e",
   "metadata": {},
   "source": [
    "## Model Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3e8b2c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import argparse\n",
    "\n",
    "def add_base_options(parser):\n",
    "    group = parser.add_argument_group('base')\n",
    "    group.add_argument(\"--cuda\", default=True, type=bool, help=\"Use cuda device, otherwise use CPU.\")\n",
    "    group.add_argument(\"--device\", default=0, type=int, help=\"Device id to use.\")\n",
    "    group.add_argument(\"--seed\", default=10, type=int, help=\"For fixing random seed.\")\n",
    "    group.add_argument(\"--batch_size\", default=64, type=int, help=\"Batch size during training.\")\n",
    "    group.add_argument(\"--short_db\", action='store_true', help=\"Load short babel for debug.\")\n",
    "    group.add_argument(\"--cropping_sampler\", action='store_true', help=\"Load short babel for debug.\")\n",
    "    \n",
    "def add_data_options(parser):\n",
    "    group = parser.add_argument_group('dataset')\n",
    "    group.add_argument(\"--dataset\", default='humanml', choices=['humanml', 'amass', 'babel'], type=str,\n",
    "                       help=\"Dataset name (choose from list).\")\n",
    "    group.add_argument(\"--data_dir\", default=\"\", type=str,\n",
    "                       help=\"If empty, will use defaults according to the specified dataset.\")\n",
    "    \n",
    "def add_model_options(parser):\n",
    "    group = parser.add_argument_group('model')\n",
    "    group.add_argument(\"--arch\", default='trans_enc',\n",
    "                       choices=['trans_enc', 'trans_dec', 'gru'], type=str,\n",
    "                       help=\"Architecture types as reported in the paper.\")\n",
    "    group.add_argument(\"--emb_trans_dec\", default=False, type=bool,\n",
    "                       help=\"For trans_dec architecture only, if true, will inject condition as a class token\"\n",
    "                            \" (in addition to cross-attention).\")\n",
    "    group.add_argument(\"--layers\", default=8, type=int,\n",
    "                       help=\"Number of layers.\")\n",
    "    group.add_argument(\"--latent_dim\", default=512, type=int,\n",
    "                       help=\"Transformer/GRU width.\")\n",
    "    group.add_argument(\"--cond_mask_prob\", default=.1, type=float,\n",
    "                       help=\"The probability of masking the condition during training.\"\n",
    "                            \" For classifier-free guidance learning.\")\n",
    "    group.add_argument(\"--lambda_rcxyz\", default=0.0, type=float, help=\"Joint positions loss.\")\n",
    "    group.add_argument(\"--lambda_vel\", default=0.0, type=float, help=\"Joint velocity loss.\")\n",
    "    group.add_argument(\"--lambda_fc\", default=0.0, type=float, help=\"Foot contact loss.\")\n",
    "    group.add_argument(\"--use_tta\", action='store_true', help=\"Time To Arrival position encoding\")  # FIXME REMOVE?\n",
    "    group.add_argument(\"--concat_trans_emb\", action='store_true', help=\"Concat transition emb, else append after linear\")  # FIXME REMOVE?\n",
    "    group.add_argument(\"--trans_emb\", action='store_true', help=\"Allow transition embedding\")  # FIXME REMOVE?\n",
    "    \n",
    "    group.add_argument(\"--context_len\", default=0, type=int, help=\"If larger than 0, will do prefix completion.\")\n",
    "    group.add_argument(\"--pred_len\", default=0, type=int, help=\"If context_len larger than 0, will do prefix completion. If pred_len will not be specified - will use the same length as context_len\")\n",
    "    \n",
    "def add_diffusion_options(parser):\n",
    "    group = parser.add_argument_group('diffusion')\n",
    "    group.add_argument(\"--noise_schedule\", default='cosine', choices=['linear', 'cosine'], type=str,\n",
    "                       help=\"Noise schedule type\")\n",
    "    group.add_argument(\"--diffusion_steps\", default=1000, type=int,\n",
    "                       help=\"Number of diffusion steps (denoted T in the paper)\")\n",
    "    group.add_argument(\"--sigma_small\", default=True, type=bool, help=\"Use smaller sigma values.\")\n",
    "    \n",
    "def add_training_options(parser):\n",
    "    group = parser.add_argument_group('training')\n",
    "    group.add_argument(\"--save_dir\", required=False, default=\"/workspace/priorMD/temporary_folder/test_our_chatgptData\", type=str,\n",
    "                       help=\"Path to save checkpoints and results.\")\n",
    "    group.add_argument(\"--overwrite\", action='store_true',\n",
    "                       help=\"If True, will enable to use an already existing save_dir.\")\n",
    "    group.add_argument(\"--train_platform_type\", default='NoPlatform', choices=['NoPlatform', 'ClearmlPlatform', 'TensorboardPlatform'], type=str,\n",
    "                       help=\"Choose platform to log results. NoPlatform means no logging.\")\n",
    "    group.add_argument(\"--lr\", default=1e-4, type=float, help=\"Learning rate.\")\n",
    "    group.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Optimizer weight decay.\")\n",
    "    group.add_argument(\"--lr_anneal_steps\", default=0, type=int, help=\"Number of learning rate anneal steps.\")\n",
    "    group.add_argument(\"--eval_batch_size\", default=32, type=int,\n",
    "                       help=\"Batch size during evaluation loop. Do not change this unless you know what you are doing. \"\n",
    "                            \"T2m precision calculation is based on fixed batch size 32.\")\n",
    "    group.add_argument(\"--eval_split\", default='test', choices=['val', 'test'], type=str,\n",
    "                       help=\"Which split to evaluate on during training.\")\n",
    "    group.add_argument(\"--eval_during_training\", action='store_true',\n",
    "                       help=\"If True, will run evaluation during training.\")\n",
    "    group.add_argument(\"--eval_rep_times\", default=3, type=int,\n",
    "                       help=\"Number of repetitions for evaluation loop during training.\")\n",
    "    group.add_argument(\"--eval_num_samples\", default=1_000, type=int,\n",
    "                       help=\"If -1, will use all samples in the specified split.\")\n",
    "    group.add_argument(\"--log_interval\", default=1_000, type=int,\n",
    "                       help=\"Log losses each N steps\")\n",
    "    group.add_argument(\"--save_interval\", default=6_000, type=int,\n",
    "                       help=\"Save checkpoints and run evaluation each N steps\")\n",
    "    group.add_argument(\"--num_steps\", default=300_000, type=int,\n",
    "                       help=\"Training will stop after the specified number of steps.\")\n",
    "    group.add_argument(\"--num_frames\", default=120, type=int,\n",
    "                       help=\"Limit for the maximal number of frames. In HumanML3D and KIT this field is ignored.\")\n",
    "    group.add_argument(\"--resume_checkpoint\", default=\"\", type=str,\n",
    "                       help=\"If not empty, will start from the specified checkpoint (path to model###.pt file).\")\n",
    "    \n",
    "def add_frame_sampler_options(parser):\n",
    "    group = parser.add_argument_group('framesampler')\n",
    "    group.add_argument(\"--min_seq_len\", default=45, type=int,\n",
    "                       help=\"babel dataset FrameSampler minimum length\")\n",
    "    group.add_argument(\"--max_seq_len\", default=250, type=int,\n",
    "                       help=\"babel dataset FrameSampler maximum length\")\n",
    "\n",
    "parser = ArgumentParser()\n",
    "add_base_options(parser)\n",
    "add_data_options(parser)\n",
    "add_model_options(parser)\n",
    "add_diffusion_options(parser)\n",
    "add_training_options(parser)\n",
    "add_frame_sampler_options(parser)\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "#args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a68bc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import clip\n",
    "import os\n",
    "#from priorMD.model.rotation2xyz import Rotation2xyz\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, njoints, nfeats, num_actions, translation, pose_rep, glob, glob_rot,\n",
    "                 latent_dim=512, ff_size=1024, num_layers=8, num_heads=4, dropout=0.1,\n",
    "                 ablation=None, activation=\"gelu\", legacy=False, data_rep='rot6d', dataset='amass', clip_dim=512,\n",
    "                 arch='trans_enc', emb_trans_dec=False, clip_version=None, **kargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.legacy = legacy\n",
    "        #self.modeltype = modeltype\n",
    "        self.njoints = njoints\n",
    "        self.nfeats = nfeats\n",
    "        self.num_actions = num_actions\n",
    "        self.data_rep = data_rep\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.pose_rep = pose_rep\n",
    "        self.glob = glob\n",
    "        self.glob_rot = glob_rot\n",
    "        self.translation = translation\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.ff_size = ff_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.ablation = ablation\n",
    "        self.activation = activation\n",
    "        self.clip_dim = clip_dim\n",
    "        self.action_emb = kargs.get('action_emb', None)\n",
    "\n",
    "        self.input_feats = self.njoints * self.nfeats\n",
    "\n",
    "        self.normalize_output = kargs.get('normalize_encoder_output', False)\n",
    "\n",
    "        self.cond_mode = 'text'# kargs.get('cond_mode', 'no_cond')\n",
    "        self.cond_mask_prob = kargs.get('cond_mask_prob', 0.)\n",
    "        self.arch = arch\n",
    "        self.gru_emb_dim = self.latent_dim if self.arch == 'gru' else 0\n",
    "        self.Cam_input_process = CamInputProcess(self.data_rep, self.input_feats+self.gru_emb_dim, self.latent_dim)\n",
    "        self.input_process = InputProcess(self.data_rep, self.input_feats+self.gru_emb_dim, self.latent_dim)\n",
    "        \n",
    "        self.sequence_pos_encoder = PositionalEncoding(self.latent_dim, self.dropout)\n",
    "        self.cond_pos_encoder = PositionalEncoding(self.latent_dim, self.dropout)\n",
    "        self.cam_sequence_pos_encoder = CamPositionalEncoding(self.latent_dim, self.dropout)\n",
    "        self.emb_trans_dec = emb_trans_dec\n",
    "        \n",
    "        self.Camera_input_process = nn.Linear(6, self.latent_dim)\n",
    "        self.Camera_output_process = nn.Linear(self.latent_dim, 6)\n",
    "        \n",
    "        self.encode_img = nn.Linear(768, 512)\n",
    "        self.encode_output = nn.Linear(1536, 512)\n",
    "        self.cond_proj = nn.Linear(512, 512)\n",
    "        self.ada_mlp = nn.Linear(1024, 1024)\n",
    "        self.ln = nn.LayerNorm(latent_dim)\n",
    "        self.mlp = nn.Linear(512, 512)\n",
    "        \n",
    "        self.camera_person = CameraPersonBlock(arch=\"trans_enc\",\n",
    "                                             fn_type='in_both_out_cur',\n",
    "                                             num_layers=2,\n",
    "                                             latent_dim=self.latent_dim,\n",
    "                                             input_feats=self.input_feats,\n",
    "                                             predict_6dof=True)\n",
    "\n",
    "        if self.arch == 'trans_enc':\n",
    "            #assert 0 < self.args.multi_backbone_split <= self.num_layers\n",
    "            print(f'CUTTING BACKBONE AT LAYER 8')\n",
    "            seqTransEncoderLayer = nn.TransformerEncoderLayer(d_model=self.latent_dim,\n",
    "                                                              nhead=self.num_heads,\n",
    "                                                              dim_feedforward=self.ff_size,\n",
    "                                                              dropout=self.dropout,\n",
    "                                                              activation=self.activation)\n",
    "            \n",
    "            CamTransEncoderLayer = nn.TransformerEncoderLayer(d_model=self.latent_dim,\n",
    "                                                              nhead=self.num_heads,\n",
    "                                                              dim_feedforward=self.ff_size,\n",
    "                                                              dropout=self.dropout,\n",
    "                                                              activation=self.activation)\n",
    "\n",
    "            self.CamTransEncoder = nn.TransformerEncoder(CamTransEncoderLayer,\n",
    "                                                         num_layers=self.num_layers)\n",
    "        \n",
    "            self.seqTransEncoder_start = nn.TransformerEncoder(seqTransEncoderLayer,\n",
    "                                                               num_layers=8)\n",
    "            self.seqTransEncoder_end = nn.TransformerEncoder(seqTransEncoderLayer,\n",
    "                                                             num_layers=self.num_layers - 8)\n",
    "\n",
    "        self.embed_timestep = TimestepEmbedder(self.latent_dim, self.sequence_pos_encoder)\n",
    "\n",
    "        if self.cond_mode != 'no_cond':\n",
    "            if 'text' in self.cond_mode:\n",
    "                self.embed_text = nn.Linear(self.clip_dim, self.latent_dim)\n",
    "                print('EMBED TEXT')\n",
    "                print('Loading CLIP...')\n",
    "               # self.clip_version = clip_version\n",
    "               # self.clip_model = self.load_and_freeze_clip(clip_version)\n",
    "            if 'action' in self.cond_mode:\n",
    "                self.embed_action = EmbedAction(self.num_actions, self.latent_dim)\n",
    "                print('EMBED ACTION')\n",
    "\n",
    "        self.Cam_output_process = CamOutputProcess(self.data_rep, self.input_feats, self.latent_dim, self.njoints,\n",
    "                                            self.nfeats)\n",
    "\n",
    "        #self.rot2xyz = Rotation2xyz(device='cpu', dataset=self.dataset)\n",
    "        self.series_model = TimeSeriesTransformer(input_dim=150, embed_dim=latent_dim, num_heads=8, num_layers=4)\n",
    "        \n",
    "        self.motion_cross_attn = nn.MultiheadAttention(latent_dim, num_heads=4, batch_first=True)\n",
    "        self.cross_attn_x_to_y = nn.MultiheadAttention(latent_dim, num_heads=4, batch_first=True)\n",
    "        self.motion_norm = nn.LayerNorm(latent_dim)\n",
    "\n",
    "    def parameters_wo_clip(self):\n",
    "        return [p for name, p in self.named_parameters() if not name.startswith('clip_model.')]\n",
    "\n",
    "    def load_and_freeze_clip(self, clip_version):\n",
    "        clip_model, clip_preprocess = clip.load(clip_version, device='cpu',\n",
    "                                                jit=False)  # Must set jit=False for training\n",
    "        clip.model.convert_weights(\n",
    "            clip_model)  # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "        # Freeze CLIP weights\n",
    "        clip_model.eval()\n",
    "        for p in clip_model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        return clip_model\n",
    "\n",
    "    def mask_cond(self, cond, force_mask=False):\n",
    "        bs, d = cond.shape\n",
    "        if force_mask:\n",
    "            return torch.zeros_like(cond)\n",
    "        elif self.training and self.cond_mask_prob > 0.:\n",
    "            mask = torch.bernoulli(torch.ones(bs, device=cond.device) * self.cond_mask_prob).view(bs, 1)  # 1-> use null_cond, 0-> use real cond\n",
    "            return cond * (1. - mask)\n",
    "        else:\n",
    "            return cond\n",
    "\n",
    "    def encode_text(self, raw_text):\n",
    "        # raw_text - list (batch_size length) of strings with input text prompts\n",
    "        device = next(self.parameters()).device\n",
    "        max_text_len = 20 if self.dataset in ['humanml', 'kit'] else None  # Specific hardcoding for humanml dataset\n",
    "        if max_text_len is not None:\n",
    "            default_context_length = 77\n",
    "            context_length = max_text_len + 2 # start_token + 20 + end_token\n",
    "            assert context_length < default_context_length\n",
    "            texts = clip.tokenize(raw_text, context_length=context_length, truncate=True).to(device) # [bs, context_length] # if n_tokens > context_length -> will truncate\n",
    "            # print('texts', texts.shape)\n",
    "            zero_pad = torch.zeros([texts.shape[0], default_context_length-context_length], dtype=texts.dtype, device=texts.device)\n",
    "            texts = torch.cat([texts, zero_pad], dim=1)\n",
    "            # print('texts after pad', texts.shape, texts)\n",
    "        else:\n",
    "            texts = clip.tokenize(raw_text, truncate=True).to(device) # [bs, context_length] # if n_tokens > 77 -> will truncate\n",
    "        return self.clip_model.encode_text(texts).float()\n",
    "\n",
    "    def forward(self, x, timesteps, y=None):\n",
    "        #print(x.shape)\n",
    "        \"\"\"\n",
    "        x: [batch_size, njoints, nfeats, max_frames], denoted x_t in the paper\n",
    "        timesteps: [batch_size] (int)\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(y.shape)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        \n",
    "        bs, njoints, nfeats, nframes = x.shape\n",
    "        emb = self.embed_timestep(timesteps)  # [1, bs, d\n",
    "        \n",
    "        #seq1 = y[:,:,0,:]\n",
    "        #seq2 = y[:,:,1,:]\n",
    "        \n",
    "        x_cur = self.input_process(y) #[seqlen, bs, d]\n",
    "       # x_other = self.input_process(seq2.unsqueeze(2))\n",
    "        x_camera = self.Cam_input_process(x)\n",
    "        #print(x_camera.shape)\n",
    "\n",
    "        #print(emb.shape, x.shape, x_other.shape, x_camera.shape)\n",
    "\n",
    "        low_x = x_cur\n",
    "        low_x_camera = x_camera\n",
    "        #print(emb.shape,x.shape)\n",
    "\n",
    "        # adding the timestep embed\n",
    "        xseq = torch.cat((emb, x_cur), axis=0)  # [seqlen+1, bs, d]\n",
    "        xseq = self.sequence_pos_encoder(xseq)  # [seqlen+1, bs, d]\n",
    "        #x_other = torch.cat((emb, x_other), axis=0)\n",
    "        #x_other = self.sequence_pos_encoder(x_other)\n",
    "        x_camera = torch.cat((emb, x_camera), axis=0)\n",
    "        x_camera = self.cam_sequence_pos_encoder(x_camera)\n",
    "\n",
    "\n",
    "        mid = self.seqTransEncoder_start(xseq)[1:]\n",
    "        #mid_other = self.seqTransEncoder_start(x_other)[1:]\n",
    "        mid_Camera = self.CamTransEncoder(x_camera)[1:]\n",
    "\n",
    "\n",
    "        delta_camera_1, delta_x_1 = self.camera_person(low_cur=low_x_camera, \n",
    "                                                        low_other=low_x, \n",
    "                                                       cur=mid_Camera, \n",
    "                                                        other=mid)\n",
    "\n",
    "        #delta_camera_2, delta_x_other_1 = self.camera_person(low_cur=low_x_camera, \n",
    "                                                           # low_other=low_x_other, \n",
    "                                                          #  cur=mid_Camera, \n",
    "                                                           # other=mid_other)\n",
    "\n",
    "        mid_Camera += delta_camera_1           \n",
    "\n",
    "        output_camera = mid_Camera\n",
    "            #print(\"222222222\")\n",
    "\n",
    "        #print(output_x.shape, output_other.shape, output_camera.shape)\n",
    "\n",
    "        output_camera = self.Cam_output_process(output_camera)\n",
    "        \n",
    "        return output_camera\n",
    "\n",
    "\n",
    "    #def _apply(self, fn):\n",
    "        #super()._apply(fn)\n",
    "        #self.rot2xyz.smpl_model._apply(fn)\n",
    "\n",
    "\n",
    "    def train(self, *args, **kwargs):\n",
    "        super().train(*args, **kwargs)\n",
    "        #self.rot2xyz.smpl_model.train(*args, **kwargs)\n",
    "\n",
    "        \n",
    "# ---- Learnable Positional Encoding ----\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embed[:, :x.size(1)]\n",
    "\n",
    "# ---- Transformer Encoder for Time Series ----\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=512, num_heads=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.pos_encoding = LearnablePositionalEncoding(embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=1024, dropout=0.1, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.norm(x)\n",
    "    \n",
    "    \n",
    "class CamPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(CamPositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # not used in the final model\n",
    "        x = x + self.pe[:x.shape[0], :]\n",
    "        return self.dropout(x)\n",
    "        \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # not used in the final model\n",
    "        x = x + self.pe[:x.shape[0], :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    def __init__(self, latent_dim, sequence_pos_encoder):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sequence_pos_encoder = sequence_pos_encoder\n",
    "\n",
    "        time_embed_dim = self.latent_dim\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, timesteps):\n",
    "        return self.time_embed(self.sequence_pos_encoder.pe[timesteps]).permute(1, 0, 2)\n",
    "    \n",
    "class CameraPersonBlock(nn.Module):\n",
    "    def __init__(self, arch, fn_type, num_layers, latent_dim, input_feats, predict_6dof):\n",
    "        super().__init__()\n",
    "        self.arch = arch\n",
    "        self.fn_type = fn_type\n",
    "        self.predict_6dof = predict_6dof\n",
    "        self.num_layers = num_layers\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = 4\n",
    "        self.ff_size = 1024\n",
    "        self.dropout = 0.1\n",
    "        self.activation = 'gelu'\n",
    "        self.input_feats = input_feats\n",
    "        if self.predict_6dof:\n",
    "            self.canon_agg = nn.Linear(9*2, self.latent_dim)\n",
    "            # self.canon_agg = nn.Linear(9*2, self.latent_dim)\n",
    "            # self.canon_agg = nn.Linear(self.input_feats*2, self.latent_dim)\n",
    "            self.canon_out = nn.Linear(self.latent_dim, 9)\n",
    "            # self.canon_out = nn.Linear(self.latent_dim, self.input_feats)\n",
    "        if 'in_both' in self.fn_type:\n",
    "            self.aggregation = nn.Linear(self.latent_dim*2, self.latent_dim)\n",
    "        if self.arch == 'trans_enc':\n",
    "            seqTransEncoderLayer = nn.TransformerEncoderLayer(d_model=self.latent_dim,\n",
    "                                                              nhead=self.num_heads,\n",
    "                                                              dim_feedforward=self.ff_size,\n",
    "                                                              dropout=self.dropout,\n",
    "                                                              activation=self.activation)\n",
    "            self.model = nn.TransformerEncoder(seqTransEncoderLayer,\n",
    "                                               num_layers=self.num_layers)\n",
    "        self.cross_attention = CrossAttention(embed_size=512, heads=4) \n",
    "        self.con_global = nn.Linear(self.latent_dim, 9)\n",
    "        self.avg_pooling = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.max_pooling = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "    def forward(self, low_cur=None, low_other=None, other=None, cur=None):     \n",
    "        low_x, low_x_other = low_cur + cur, low_other + other\n",
    "        # 交换concat顺序计算向量\n",
    "        x = self.aggregation(torch.concatenate((low_x, low_x_other), dim=-1))\n",
    "        x_other = self.aggregation(torch.concatenate((low_x_other, low_x), dim=-1))\n",
    "        # print(\"COMDMD中的x维度:\", x.shape) torch.Size([120, 64, 512])\n",
    "        x_out = self.model(x)# torch.Size([120, 64, 512])\n",
    "        x_other_out= self.model(x_other)# torch.Size([120, 64, 512])\n",
    "        # print(\"文本向量的emb:\", text_emb.shape) torch.Size([1, 64, 512])\n",
    "        # cross attention \n",
    "        #xA_cross, _ = self.cross_attn_B2A(x_out, x_other_out, x_other_out)\n",
    "        #xB_cross, _ = self.cross_attn_A2B(x_other_out, x_out, x_out)\n",
    "\n",
    "        return x_out, x_other_out\n",
    "\n",
    "\n",
    "class CamInputProcess(nn.Module):\n",
    "    def __init__(self, data_rep, input_feats, latent_dim):\n",
    "        super().__init__()\n",
    "        self.data_rep = data_rep\n",
    "        self.input_feats = input_feats\n",
    "        self.latent_dim = latent_dim\n",
    "        self.poseEmbedding = nn.Linear(self.input_feats, self.latent_dim)\n",
    "        if self.data_rep == 'rot_vel':\n",
    "            self.velEmbedding = nn.Linear(self.input_feats, self.latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, njoints, nfeats, nframes = x.shape\n",
    "        x = x.permute((3, 0, 1, 2)).reshape(nframes, bs, njoints*nfeats)\n",
    "\n",
    "        if self.data_rep in ['rot6d', 'xyz', 'hml_vec']:\n",
    "            x = self.poseEmbedding(x)  # [seqlen, bs, d]\n",
    "            return x\n",
    "        elif self.data_rep == 'rot_vel':\n",
    "            first_pose = x[[0]]  # [1, bs, 150]\n",
    "            first_pose = self.poseEmbedding(first_pose)  # [1, bs, d]\n",
    "            vel = x[1:]  # [seqlen-1, bs, 150]\n",
    "            vel = self.velEmbedding(vel)  # [seqlen-1, bs, d]\n",
    "            return torch.cat((first_pose, vel), axis=0)  # [seqlen, bs, d]\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "\n",
    "class CamOutputProcess(nn.Module):\n",
    "    def __init__(self, data_rep, input_feats, latent_dim, njoints, nfeats):\n",
    "        super().__init__()\n",
    "        self.data_rep = data_rep\n",
    "        self.input_feats = input_feats\n",
    "        self.latent_dim = latent_dim\n",
    "        self.njoints = njoints\n",
    "        self.nfeats = nfeats\n",
    "        self.poseFinal = nn.Linear(self.latent_dim, self.input_feats)\n",
    "        if self.data_rep == 'rot_vel':\n",
    "            self.velFinal = nn.Linear(self.latent_dim, self.input_feats)\n",
    "\n",
    "    def forward(self, output):\n",
    "        nframes, bs, d = output.shape\n",
    "        if self.data_rep in ['rot6d', 'xyz', 'hml_vec']:\n",
    "            output = self.poseFinal(output)  # [seqlen, bs, 150]\n",
    "        elif self.data_rep == 'rot_vel':\n",
    "            first_pose = output[[0]]  # [1, bs, d]\n",
    "            first_pose = self.poseFinal(first_pose)  # [1, bs, 150]\n",
    "            vel = output[1:]  # [seqlen-1, bs, d]\n",
    "            vel = self.velFinal(vel)  # [seqlen-1, bs, 150]\n",
    "            output = torch.cat((first_pose, vel), axis=0)  # [seqlen, bs, 150]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        output = output.reshape(nframes, bs, self.njoints, self.nfeats)\n",
    "        output = output.permute(1, 2, 3, 0)  # [bs, njoints, nfeats, nframes]\n",
    "        return output\n",
    "    \n",
    "class InputProcess(nn.Module):\n",
    "    def __init__(self, data_rep, input_feats, latent_dim):\n",
    "        super().__init__()\n",
    "        self.data_rep = data_rep\n",
    "        self.input_feats = input_feats\n",
    "        self.latent_dim = latent_dim\n",
    "        self.poseEmbedding = nn.Linear(self.input_feats, self.latent_dim)\n",
    "        if self.data_rep == 'rot_vel':\n",
    "            self.velEmbedding = nn.Linear(self.input_feats, self.latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, njoints, nfeats, nframes = x.shape\n",
    "        x = x.permute((3, 0, 1, 2)).reshape(nframes, bs, njoints*nfeats)\n",
    "\n",
    "        if self.data_rep in ['rot6d', 'xyz', 'hml_vec']:\n",
    "            x = self.poseEmbedding(x)  # [seqlen, bs, d]\n",
    "            return x\n",
    "        elif self.data_rep == 'rot_vel':\n",
    "            first_pose = x[[0]]  # [1, bs, 150]\n",
    "            first_pose = self.poseEmbedding(first_pose)  # [1, bs, d]\n",
    "            vel = x[1:]  # [seqlen-1, bs, 150]\n",
    "            vel = self.velEmbedding(vel)  # [seqlen-1, bs, d]\n",
    "            return torch.cat((first_pose, vel), axis=0)  # [seqlen, bs, d]\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "\n",
    "class OutputProcess(nn.Module):\n",
    "    def __init__(self, data_rep, input_feats, latent_dim, njoints, nfeats):\n",
    "        super().__init__()\n",
    "        self.data_rep = data_rep\n",
    "        self.input_feats = input_feats\n",
    "        self.latent_dim = latent_dim\n",
    "        self.njoints = njoints\n",
    "        self.nfeats = nfeats\n",
    "        self.poseFinal = nn.Linear(self.latent_dim, self.input_feats)\n",
    "        if self.data_rep == 'rot_vel':\n",
    "            self.velFinal = nn.Linear(self.latent_dim, self.input_feats)\n",
    "\n",
    "    def forward(self, output):\n",
    "        nframes, bs, d = output.shape\n",
    "        if self.data_rep in ['rot6d', 'xyz', 'hml_vec']:\n",
    "            output = self.poseFinal(output)  # [seqlen, bs, 150]\n",
    "        elif self.data_rep == 'rot_vel':\n",
    "            first_pose = output[[0]]  # [1, bs, d]\n",
    "            first_pose = self.poseFinal(first_pose)  # [1, bs, 150]\n",
    "            vel = output[1:]  # [seqlen-1, bs, d]\n",
    "            vel = self.velFinal(vel)  # [seqlen-1, bs, 150]\n",
    "            output = torch.cat((first_pose, vel), axis=0)  # [seqlen, bs, 150]\n",
    "        else:\n",
    "            raise ValueError\n",
    "        output = output.reshape(nframes, bs, self.njoints, self.nfeats)\n",
    "        output = output.permute(1, 2, 3, 0)  # [bs, njoints, nfeats, nframes]\n",
    "        return output\n",
    "\n",
    "\n",
    "class EmbedAction(nn.Module):\n",
    "    def __init__(self, num_actions, latent_dim):\n",
    "        super().__init__()\n",
    "        self.action_embedding = nn.Parameter(torch.randn(num_actions, latent_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        idx = input[:, 0].to(torch.long)  # an index array must be long\n",
    "        output = self.action_embedding[idx]\n",
    "        return output\n",
    "    \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        ori_v = values\n",
    "        values, keys, query = values.permute(1,0,2),keys.permute(1,0,2), query.permute(1,0,2)\n",
    "        query = torch.repeat_interleave(query, repeats=values.shape[1],dim=1)\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Attention mechanism\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        out =  out.permute(1,0,2) + ori_v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd080a",
   "metadata": {},
   "source": [
    "## Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64228ce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUTTING BACKBONE AT LAYER 8\n",
      "EMBED TEXT\n",
      "Loading CLIP...\n"
     ]
    }
   ],
   "source": [
    "from diffusion.respace import SpacedDiffusion, space_timesteps\n",
    "from utils.model_util import create_gaussian_diffusion\n",
    "from diffusion.gaussian_diffusion import (\n",
    "    GaussianDiffusion,\n",
    "    get_named_beta_schedule,\n",
    "    create_named_schedule_sampler,\n",
    "    ModelMeanType,\n",
    "    ModelVarType,\n",
    "    LossType\n",
    ")\n",
    "\n",
    "model = DiffusionModel(njoints=79, nfeats=1, num_actions=1, translation=True, pose_rep=\"rot6d\", glob=True, glob_rot=True).to(\"cuda\")\n",
    "\n",
    "sampler_name = 'uniform'\n",
    "beta_scheduler = 'linear'\n",
    "betas = get_named_beta_schedule(beta_scheduler, args.diffusion_steps)\n",
    "diffusion = GaussianDiffusion(\n",
    "            betas=betas,\n",
    "            model_mean_type=ModelMeanType.EPSILON,\n",
    "            model_var_type=ModelVarType.FIXED_SMALL,\n",
    "            loss_type=LossType.MSE\n",
    "        )\n",
    "sampler = create_named_schedule_sampler(sampler_name, diffusion)\n",
    "\n",
    "n_epoch = 10000\n",
    "batch_size = 32\n",
    "n_T = 1000 # 500\n",
    "device = \"cuda\"\n",
    "n_feature = 5\n",
    "n_textemb = 512\n",
    "lrate = 0.0001\n",
    "save_model = True\n",
    "save_dir = './weight/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "optim = torch.optim.Adam(model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5118cde",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a729f088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1547: 100%|███████████████████████████| 309/309 [00:41<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved model at ./weight/conductor_model_0.pth\n",
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1433:  10%|██▋                         | 30/309 [00:04<00:42,  6.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         loss_ema \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m\u001b[38;5;241m*\u001b[39mloss_ema\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0.05\u001b[39m\u001b[38;5;241m*\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_ema\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), save_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconductor_latest.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_model \u001b[38;5;129;01mand\u001b[39;00m ep \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/torch/optim/adam.py:703\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    701\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 703\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    706\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    print(f'epoch {ep}')\n",
    "    model.train()\n",
    "\n",
    "    # linear lrate decay\n",
    "    optim.param_groups[0]['lr'] = 0.0001*(1-ep/n_epoch)\n",
    "\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    loss_ema = None\n",
    "    for x in pbar:\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        motion = x[1].to(device).to(torch.float32)\n",
    "        audio = x[0].to(device).to(torch.float32)\n",
    "        \n",
    "        img_dic = {\"y\":audio}\n",
    "        \n",
    "        c = torch.zeros([64,512])\n",
    "        t, _ = sampler.sample(motion.shape[0], device)\n",
    "        \n",
    "\n",
    "        loss = diffusion.training_losses(\n",
    "                                            model=model,\n",
    "                                            x_start=motion,\n",
    "                                            t=t,\n",
    "                                            model_kwargs=img_dic\n",
    "                                            #model_kwargs={}\n",
    "                                        )[\"mse\"].to(torch.float32)\n",
    "        loss = torch.mean(loss)\n",
    "        loss.backward()\n",
    "        if loss_ema is None:\n",
    "            loss_ema = loss.item()\n",
    "        else:\n",
    "            loss_ema = 0.95*loss_ema+0.05*loss.item()\n",
    "        pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "        optim.step()\n",
    "\n",
    "    torch.save(model.state_dict(), save_dir + f\"conductor_latest.pth\")\n",
    "    if save_model and ep % 100 == 0:\n",
    "        torch.save(model.state_dict(), save_dir + f\"conductor_model_{ep}.pth\")\n",
    "        print('saved model at ' + save_dir + f\"conductor_model_{ep}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5e2403f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 138, 1, 120])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb = audio_tensor[::1000].to(\"cuda\").to(torch.float32)\n",
    "test_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2395e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47897060",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_list=[]\n",
    "A_Mean = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Audio_Mean.npy\")\n",
    "A_Std = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Audio_Std.npy\")\n",
    "for audio_file in os.listdir(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Mel_Clips/liu/\"):\n",
    "    if(audio_file.split(\"_\")[1]==\"1.npy\"):\n",
    "        test_audio_list.append((np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Mel_Clips/liu/\"+audio_file)-A_Mean)/(A_Std+0.000000001))\n",
    "        #print(audio_file)\n",
    "test_audio_tensor = torch.tensor(np.array(test_audio_list))\n",
    "#test_audio_tensor = F.pad(test_audio_tensor, (0, 74), mode=\"constant\", value=0)\n",
    "test_audio_tensor = test_audio_tensor.reshape(-1,1,120,64).permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "964bd9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_beat_list=[]\n",
    "B_Mean = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Beat_Mean.npy\")\n",
    "B_Std = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Beat_Std.npy\")\n",
    "for audio_file in os.listdir(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Beat_Clips/liu/\"):\n",
    "    if(audio_file.split(\"_\")[1]==\"1.npy\"):\n",
    "        test_beat_list.append((np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Beat_Clips/liu/\"+audio_file)-B_Mean)/(B_Std+0.000000001))\n",
    "        #print(audio_file)\n",
    "test_beat_tensor = torch.tensor(np.array(test_beat_list))\n",
    "#test_beat_tensor = F.pad(test_beat_tensor, (0, 33), mode=\"constant\", value=0)\n",
    "test_beat_tensor = test_beat_tensor.reshape(-1,1,120,15).permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42d53d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_tensor = torch.concatenate([test_audio_tensor,test_beat_tensor], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30f0b69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727073826d404c52bb2865fef08d5453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"weight/conductor_beat_mel_model_500.pth\", map_location=torch.device('cuda')))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    test_emb = test_full_tensor[20*i:20*i+20].to(\"cuda\").to(torch.float32)\n",
    "    test_dic = {\"y\":test_emb}\n",
    "    \n",
    "    test_array = diffusion.p_sample_loop(\n",
    "                model,\n",
    "                (20, 79, 1,120),\n",
    "                clip_denoised=False,\n",
    "                progress=True,\n",
    "                #model_kwargs={})\n",
    "                model_kwargs=test_dic)\n",
    "    break\n",
    "    \n",
    "    #np.save(\"Evaluation/beat_\"+str(i)+\".npy\",np.array(test_array.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08a1cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "Mean = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Motion_Mean.npy\")\n",
    "Std = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Motion_Std.npy\")\n",
    "\n",
    "\n",
    "#Mean = np.load(\"../Camera/Mix_Mean.npy\")\n",
    "#Std = np.load(\"../Camera/Mix_Std.npy\")\n",
    "#D_mean = np.load(\"../Camera/Mix_D_Mean.npy\")\n",
    "#D_std = np.load(\"../Camera/Mix_D_Std.npy\")\n",
    "\n",
    "test_data = test_array.cpu().detach().numpy().transpose(2,0,3,1)\n",
    "motion1 = test_data[0][:,:,:48].reshape(-1,120,8,6)\n",
    "\n",
    "m = motion1[4]\n",
    "\n",
    "arr_full = np.zeros((120, 23, 6))\n",
    "\n",
    "target_idx = [13, 14, 16, 17, 18, 19, 20, 21]   # 对应 13,14,16,17,18,19,20,21\n",
    "\n",
    "# 1) 把 (120,8,6) 填进去\n",
    "for i, t in enumerate(target_idx):\n",
    "    arr_full[:, t, :] = m[:, i, :]\n",
    "    \n",
    "arr_full = arr_full*(Std)+Mean\n",
    "\n",
    "# 2) 其他位置用 (23,6) 的值补充\n",
    "for j in range(23):\n",
    "    if j not in target_idx:\n",
    "        arr_full[:, j, :] = Mean[j]\n",
    "\n",
    "d = np.array([1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p0.npy\",arr_full)\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p1.npy\",arr_full)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p0.npy\",d)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p1.npy\",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b324bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_1_p0.npy\n",
      "2_1_p0.npy\n",
      "3_1_p0.npy\n",
      "4_1_p0.npy\n",
      "5_1_p0.npy\n",
      "6_1_p0.npy\n",
      "7_1_p0.npy\n",
      "8_1_p0.npy\n",
      "9_1_p0.npy\n",
      "10_1_p0.npy\n",
      "11_1_p0.npy\n",
      "12_1_p0.npy\n",
      "13_1_p0.npy\n",
      "14_1_p0.npy\n",
      "15_1_p0.npy\n",
      "16_1_p0.npy\n",
      "17_1_p0.npy\n",
      "18_1_p0.npy\n",
      "19_1_p0.npy\n",
      "20_1_p0.npy\n",
      "21_1_p0.npy\n",
      "22_1_p0.npy\n",
      "23_1_p0.npy\n",
      "24_1_p0.npy\n",
      "25_1_p0.npy\n",
      "26_1_p0.npy\n",
      "27_1_p0.npy\n",
      "28_1_p0.npy\n",
      "29_1_p0.npy\n",
      "30_1_p0.npy\n",
      "31_1_p0.npy\n",
      "32_1_p0.npy\n",
      "33_1_p0.npy\n",
      "34_1_p0.npy\n",
      "35_1_p0.npy\n",
      "36_1_p0.npy\n",
      "37_1_p0.npy\n",
      "38_1_p0.npy\n",
      "39_1_p0.npy\n",
      "40_1_p0.npy\n",
      "41_1_p0.npy\n",
      "42_1_p0.npy\n",
      "43_1_p0.npy\n",
      "44_1_p0.npy\n",
      "45_1_p0.npy\n",
      "46_1_p0.npy\n",
      "47_1_p0.npy\n",
      "48_1_p0.npy\n",
      "49_1_p0.npy\n",
      "50_1_p0.npy\n",
      "51_1_p0.npy\n",
      "52_1_p0.npy\n",
      "53_1_p0.npy\n",
      "54_1_p0.npy\n",
      "55_1_p0.npy\n",
      "56_1_p0.npy\n",
      "57_1_p0.npy\n",
      "58_1_p0.npy\n",
      "59_1_p0.npy\n",
      "60_1_p0.npy\n",
      "61_1_p0.npy\n",
      "62_1_p0.npy\n",
      "63_1_p0.npy\n",
      "64_1_p0.npy\n",
      "65_1_p0.npy\n",
      "66_1_p0.npy\n",
      "67_1_p0.npy\n",
      "68_1_p0.npy\n",
      "69_1_p0.npy\n",
      "70_1_p0.npy\n",
      "71_1_p0.npy\n",
      "72_1_p0.npy\n",
      "73_1_p0.npy\n",
      "74_1_p0.npy\n",
      "75_1_p0.npy\n",
      "76_1_p0.npy\n",
      "77_1_p0.npy\n",
      "78_1_p0.npy\n",
      "79_1_p0.npy\n",
      "80_1_p0.npy\n",
      "81_1_p0.npy\n",
      "82_1_p0.npy\n",
      "83_1_p0.npy\n",
      "84_1_p0.npy\n",
      "85_1_p0.npy\n",
      "86_1_p0.npy\n",
      "87_1_p0.npy\n",
      "88_1_p0.npy\n",
      "89_1_p0.npy\n",
      "90_1_p0.npy\n",
      "91_1_p0.npy\n",
      "92_1_p0.npy\n",
      "93_1_p0.npy\n",
      "94_1_p0.npy\n",
      "95_1_p0.npy\n",
      "96_1_p0.npy\n",
      "97_1_p0.npy\n",
      "98_1_p0.npy\n",
      "99_1_p0.npy\n",
      "100_1_p0.npy\n",
      "101_1_p0.npy\n",
      "102_1_p0.npy\n",
      "103_1_p0.npy\n",
      "104_1_p0.npy\n",
      "105_1_p0.npy\n",
      "106_1_p0.npy\n",
      "107_1_p0.npy\n",
      "108_1_p0.npy\n",
      "109_1_p0.npy\n",
      "110_1_p0.npy\n",
      "111_1_p0.npy\n",
      "112_1_p0.npy\n",
      "113_1_p0.npy\n",
      "114_1_p0.npy\n",
      "115_1_p0.npy\n",
      "116_1_p0.npy\n",
      "117_1_p0.npy\n",
      "118_1_p0.npy\n",
      "119_1_p0.npy\n",
      "120_1_p0.npy\n",
      "121_1_p0.npy\n",
      "122_1_p0.npy\n",
      "123_1_p0.npy\n",
      "124_1_p0.npy\n",
      "125_1_p0.npy\n",
      "126_1_p0.npy\n",
      "127_1_p0.npy\n",
      "128_1_p0.npy\n",
      "129_1_p0.npy\n",
      "130_1_p0.npy\n",
      "131_1_p0.npy\n",
      "132_1_p0.npy\n",
      "133_1_p0.npy\n",
      "134_1_p0.npy\n",
      "135_1_p0.npy\n",
      "136_1_p0.npy\n",
      "137_1_p0.npy\n",
      "138_1_p0.npy\n",
      "139_1_p0.npy\n",
      "140_1_p0.npy\n",
      "141_1_p0.npy\n",
      "142_1_p0.npy\n",
      "143_1_p0.npy\n",
      "144_1_p0.npy\n",
      "145_1_p0.npy\n",
      "146_1_p0.npy\n",
      "147_1_p0.npy\n",
      "148_1_p0.npy\n",
      "149_1_p0.npy\n",
      "150_1_p0.npy\n",
      "151_1_p0.npy\n",
      "152_1_p0.npy\n",
      "153_1_p0.npy\n",
      "154_1_p0.npy\n",
      "155_1_p0.npy\n",
      "156_1_p0.npy\n",
      "157_1_p0.npy\n",
      "158_1_p0.npy\n",
      "159_1_p0.npy\n",
      "160_1_p0.npy\n",
      "161_1_p0.npy\n",
      "162_1_p0.npy\n",
      "163_1_p0.npy\n",
      "164_1_p0.npy\n",
      "165_1_p0.npy\n",
      "166_1_p0.npy\n",
      "167_1_p0.npy\n",
      "168_1_p0.npy\n",
      "169_1_p0.npy\n",
      "170_1_p0.npy\n",
      "171_1_p0.npy\n",
      "172_1_p0.npy\n",
      "173_1_p0.npy\n",
      "174_1_p0.npy\n",
      "175_1_p0.npy\n",
      "176_1_p0.npy\n",
      "177_1_p0.npy\n",
      "178_1_p0.npy\n",
      "179_1_p0.npy\n",
      "180_1_p0.npy\n",
      "181_1_p0.npy\n",
      "182_1_p0.npy\n",
      "183_1_p0.npy\n",
      "184_1_p0.npy\n",
      "185_1_p0.npy\n",
      "186_1_p0.npy\n",
      "187_1_p0.npy\n",
      "188_1_p0.npy\n",
      "189_1_p0.npy\n",
      "190_1_p0.npy\n",
      "191_1_p0.npy\n",
      "192_1_p0.npy\n",
      "193_1_p0.npy\n",
      "194_1_p0.npy\n",
      "195_1_p0.npy\n",
      "196_1_p0.npy\n",
      "197_1_p0.npy\n",
      "198_1_p0.npy\n",
      "199_1_p0.npy\n",
      "201_1_p0.npy\n",
      "202_1_p0.npy\n",
      "203_1_p0.npy\n",
      "204_1_p0.npy\n",
      "205_1_p0.npy\n",
      "206_1_p0.npy\n",
      "207_1_p0.npy\n",
      "208_1_p0.npy\n",
      "209_1_p0.npy\n",
      "210_1_p0.npy\n",
      "211_1_p0.npy\n",
      "212_1_p0.npy\n",
      "213_1_p0.npy\n",
      "214_1_p0.npy\n",
      "215_1_p0.npy\n",
      "216_1_p0.npy\n",
      "217_1_p0.npy\n",
      "218_1_p0.npy\n",
      "219_1_p0.npy\n",
      "220_1_p0.npy\n",
      "221_1_p0.npy\n",
      "222_1_p0.npy\n",
      "223_1_p0.npy\n",
      "224_1_p0.npy\n",
      "225_1_p0.npy\n",
      "226_1_p0.npy\n",
      "227_1_p0.npy\n",
      "228_1_p0.npy\n",
      "229_1_p0.npy\n",
      "230_1_p0.npy\n",
      "231_1_p0.npy\n",
      "232_1_p0.npy\n",
      "233_1_p0.npy\n",
      "234_1_p0.npy\n",
      "235_1_p0.npy\n",
      "236_1_p0.npy\n",
      "237_1_p0.npy\n",
      "238_1_p0.npy\n",
      "239_1_p0.npy\n",
      "240_1_p0.npy\n",
      "241_1_p0.npy\n",
      "242_1_p0.npy\n",
      "243_1_p0.npy\n",
      "244_1_p0.npy\n",
      "245_1_p0.npy\n",
      "246_1_p0.npy\n",
      "247_1_p0.npy\n",
      "248_1_p0.npy\n",
      "249_1_p0.npy\n",
      "250_1_p0.npy\n",
      "251_1_p0.npy\n",
      "252_1_p0.npy\n",
      "253_1_p0.npy\n",
      "254_1_p0.npy\n",
      "255_1_p0.npy\n",
      "256_1_p0.npy\n",
      "257_1_p0.npy\n",
      "258_1_p0.npy\n",
      "259_1_p0.npy\n",
      "260_1_p0.npy\n",
      "261_1_p0.npy\n",
      "262_1_p0.npy\n",
      "263_1_p0.npy\n",
      "264_1_p0.npy\n",
      "265_1_p0.npy\n",
      "266_1_p0.npy\n",
      "267_1_p0.npy\n",
      "268_1_p0.npy\n",
      "269_1_p0.npy\n",
      "270_1_p0.npy\n",
      "271_1_p0.npy\n",
      "272_1_p0.npy\n",
      "273_1_p0.npy\n",
      "274_1_p0.npy\n",
      "275_1_p0.npy\n",
      "276_1_p0.npy\n",
      "277_1_p0.npy\n",
      "278_1_p0.npy\n",
      "279_1_p0.npy\n",
      "280_1_p0.npy\n",
      "281_1_p0.npy\n",
      "282_1_p0.npy\n",
      "283_1_p0.npy\n",
      "284_1_p0.npy\n",
      "285_1_p0.npy\n",
      "286_1_p0.npy\n",
      "287_1_p0.npy\n",
      "288_1_p0.npy\n",
      "289_1_p0.npy\n",
      "290_1_p0.npy\n",
      "291_1_p0.npy\n",
      "292_1_p0.npy\n",
      "293_1_p0.npy\n",
      "294_1_p0.npy\n",
      "295_1_p0.npy\n",
      "296_1_p0.npy\n",
      "297_1_p0.npy\n",
      "298_1_p0.npy\n",
      "299_1_p0.npy\n",
      "300_1_p0.npy\n",
      "301_1_p0.npy\n",
      "302_1_p0.npy\n",
      "303_1_p0.npy\n",
      "304_1_p0.npy\n",
      "305_1_p0.npy\n",
      "306_1_p0.npy\n",
      "307_1_p0.npy\n",
      "308_1_p0.npy\n",
      "309_1_p0.npy\n",
      "310_1_p0.npy\n",
      "311_1_p0.npy\n",
      "312_1_p0.npy\n",
      "313_1_p0.npy\n",
      "314_1_p0.npy\n",
      "315_1_p0.npy\n",
      "316_1_p0.npy\n",
      "317_1_p0.npy\n",
      "318_1_p0.npy\n",
      "319_1_p0.npy\n",
      "320_1_p0.npy\n",
      "321_1_p0.npy\n",
      "322_1_p0.npy\n",
      "323_1_p0.npy\n",
      "324_1_p0.npy\n",
      "325_1_p0.npy\n",
      "326_1_p0.npy\n",
      "327_1_p0.npy\n",
      "328_1_p0.npy\n",
      "329_1_p0.npy\n",
      "330_1_p0.npy\n",
      "331_1_p0.npy\n",
      "332_1_p0.npy\n",
      "333_1_p0.npy\n",
      "334_1_p0.npy\n",
      "335_1_p0.npy\n",
      "336_1_p0.npy\n",
      "337_1_p0.npy\n",
      "338_1_p0.npy\n",
      "339_1_p0.npy\n",
      "340_1_p0.npy\n",
      "341_1_p0.npy\n",
      "342_1_p0.npy\n",
      "343_1_p0.npy\n",
      "344_1_p0.npy\n",
      "345_1_p0.npy\n",
      "346_1_p0.npy\n",
      "347_1_p0.npy\n",
      "348_1_p0.npy\n",
      "349_1_p0.npy\n",
      "350_1_p0.npy\n",
      "351_1_p0.npy\n",
      "352_1_p0.npy\n",
      "353_1_p0.npy\n",
      "354_1_p0.npy\n",
      "355_1_p0.npy\n",
      "356_1_p0.npy\n",
      "357_1_p0.npy\n",
      "358_1_p0.npy\n",
      "360_1_p0.npy\n",
      "361_1_p0.npy\n",
      "362_1_p0.npy\n",
      "363_1_p0.npy\n",
      "364_1_p0.npy\n",
      "365_1_p0.npy\n",
      "366_1_p0.npy\n",
      "367_1_p0.npy\n",
      "368_1_p0.npy\n",
      "369_1_p0.npy\n",
      "370_1_p0.npy\n",
      "371_1_p0.npy\n",
      "372_1_p0.npy\n",
      "373_1_p0.npy\n",
      "374_1_p0.npy\n",
      "375_1_p0.npy\n"
     ]
    }
   ],
   "source": [
    "gt_motion_list=[]\n",
    "for i in range(376):\n",
    "    if(str(i)+\"_1_p0.npy\" in os.listdir(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/6D_motion/liu/train/\")):\n",
    "        gt_motion_list.append(np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/6D_motion/liu/train/\"+str(i)+\"_1_p0.npy\"))\n",
    "        print(str(i)+\"_1_p0.npy\")\n",
    "gt_motion_tensor = torch.tensor(np.array(gt_motion_list))\n",
    "Mean = torch.tensor(np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Motion_Mean.npy\"))\n",
    "\n",
    "joints_to_replace = [0,1,2,3,4,5,6,7,8,9,10,11,12,15]\n",
    "\n",
    "# 遍历这些关节\n",
    "for j in joints_to_replace:\n",
    "    # 计算该关节在所有 batch、frame 上的均值 (19742,120,6) → (6,)\n",
    "    joint_mean = Mean[j, :]\n",
    "    \n",
    "    # 用均值替换该关节 (广播到 batch, frame)\n",
    "    gt_motion_tensor[:, :, j, :] = joint_mean\n",
    "\n",
    "d = np.array([1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p0.npy\",gt_motion_tensor[8])\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p1.npy\",gt_motion_tensor[8])\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p0.npy\",d)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p1.npy\",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3ad2d4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'229_3_p0.npy'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_motion_list=[]\n",
    "for motion_file in os.listdir(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/6D_motion/liu/train\"):\n",
    "    if(motion_file.split(\"_\")[1]==\"1.npy\"):\n",
    "        gt_motion_list.append(np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/6D_motion/liu/train/\"+motion_file))\n",
    "        print(motion_file)\n",
    "gt_motion_tensor = torch.tensor(np.array(gt_motion_list))\n",
    "\n",
    "\n",
    "test_data = test_array.cpu().detach().numpy().transpose(2,0,3,1)\n",
    "motion1 = test_data[0].reshape(-1,120,23,6)\n",
    "\n",
    "motion1 = motion1*(Std)+Mean\n",
    "\n",
    "d = np.array([1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p0.npy\",motion1[2])\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p1.npy\",motion1[2])\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p0.npy\",d)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p1.npy\",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5fcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0c09d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_845036/2134886861.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x1, x2 = torch.chunk(torch.tensor(test_emb[2]), chunks=2, dim=1)\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "Mean = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Motion_Mean.npy\")\n",
    "Std = np.load(\"../PoseEstimation/SMPLest-X/SMPLest-X-main/demo/CONDUCTOR_DATA/Motion_Std.npy\")\n",
    "\n",
    "\n",
    "#Mean = np.load(\"../Camera/Mix_Mean.npy\")\n",
    "#Std = np.load(\"../Camera/Mix_Std.npy\")\n",
    "#D_mean = np.load(\"../Camera/Mix_D_Mean.npy\")\n",
    "#D_std = np.load(\"../Camera/Mix_D_Std.npy\")\n",
    "\n",
    "test_data = test_array.cpu().detach().numpy()\n",
    "\n",
    "x1, x2 = torch.chunk(torch.tensor(test_emb[2]), chunks=2, dim=1)\n",
    "x1 = x1.permute(1,2,0)\n",
    "D1 = x1[0][:1,-12:].reshape(-1)[:9]\n",
    "motion1 = x1[0][:,:138].reshape(-1,23,6)\n",
    "\n",
    "x2 = x2.permute(1,2,0)\n",
    "D2 = x2[0][:1,-12:].reshape(-1)[:9]\n",
    "motion2 = x2[0][:,:138].reshape(-1,23,6)\n",
    "\n",
    "x3 = torch.tensor(test_data[7]).permute(1,2,0)[0]\n",
    "cam = x3[:,:6]\n",
    "\n",
    "motion1 = motion1.cpu()*(Std)+Mean\n",
    "motion2 = motion2.cpu()*(Std)+Mean\n",
    "    \n",
    "D1 = D1.cpu()*(D_std)+D_mean\n",
    "D2 = D2.cpu()*(D_std)+D_mean\n",
    "\n",
    "cam = cam*(C_std)+C_mean\n",
    "Cam_smooth = gaussian_filter1d(cam, sigma=2, axis=0, mode='reflect')\n",
    "\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p0.npy\",motion1)\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p1.npy\",motion2)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p0.npy\",D1)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p1.npy\",D2)\n",
    "\n",
    "np.save(\"../Camera/generated_data/camera/0.npy\",Cam_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "93b6f113",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.85840596e-01,  2.40025629e-03,  2.91814751e-01,\n",
       "        -1.40747810e-01,  4.27042831e-01, -5.65985346e-01],\n",
       "       [-2.86849678e-01,  1.36510684e-03,  2.91530954e-01,\n",
       "        -1.40796385e-01,  4.14193762e-01, -5.67888538e-01],\n",
       "       [-2.88450438e-01,  1.69550025e-04,  2.90605526e-01,\n",
       "        -1.40795028e-01,  4.04207740e-01, -5.71095772e-01],\n",
       "       [-2.89831109e-01, -1.83610852e-04,  2.88938583e-01,\n",
       "        -1.40712737e-01,  4.12597748e-01, -5.74561940e-01],\n",
       "       [-2.90293919e-01,  7.26867026e-04,  2.86987291e-01,\n",
       "        -1.40660611e-01,  4.41198820e-01, -5.77362534e-01],\n",
       "       [-2.89844211e-01,  2.53692205e-03,  2.85335636e-01,\n",
       "        -1.40708794e-01,  4.79429932e-01, -5.79215830e-01],\n",
       "       [-2.89023948e-01,  4.44944740e-03,  2.84193424e-01,\n",
       "        -1.40668644e-01,  5.13780732e-01, -5.80396990e-01],\n",
       "       [-2.88283094e-01,  5.89169372e-03,  2.83495146e-01,\n",
       "        -1.40385147e-01,  5.33962061e-01, -5.81424375e-01],\n",
       "       [-2.87828822e-01,  6.77493823e-03,  2.83369834e-01,\n",
       "        -1.40354463e-01,  5.34408823e-01, -5.82833654e-01],\n",
       "       [-2.87999742e-01,  7.04584162e-03,  2.84227485e-01,\n",
       "        -1.41649356e-01,  5.16137607e-01, -5.84882286e-01],\n",
       "       [-2.89454490e-01,  6.37197044e-03,  2.86175167e-01,\n",
       "        -1.44878204e-01,  4.87998459e-01, -5.87328832e-01],\n",
       "       [-2.92775777e-01,  4.58090854e-03,  2.88449386e-01,\n",
       "        -1.49466088e-01,  4.61877043e-01, -5.89617967e-01],\n",
       "       [-2.97730859e-01,  2.19899247e-03,  2.89787966e-01,\n",
       "        -1.54183832e-01,  4.43897096e-01, -5.91333122e-01],\n",
       "       [-3.02948576e-01,  2.47308237e-04,  2.89544473e-01,\n",
       "        -1.58059676e-01,  4.31657568e-01, -5.92475848e-01],\n",
       "       [-3.06660461e-01, -5.86632864e-04,  2.88228616e-01,\n",
       "        -1.60619493e-01,  4.20874313e-01, -5.93231309e-01],\n",
       "       [-3.08007444e-01, -5.76816857e-04,  2.86892371e-01,\n",
       "        -1.61774926e-01,  4.12805178e-01, -5.93440766e-01],\n",
       "       [-3.07572084e-01, -7.89922428e-04,  2.86229095e-01,\n",
       "        -1.62020354e-01,  4.13198647e-01, -5.92749094e-01],\n",
       "       [-3.06599655e-01, -2.20186194e-03,  2.86237629e-01,\n",
       "        -1.62404020e-01,  4.24228011e-01, -5.91370926e-01],\n",
       "       [-3.05850550e-01, -4.87571831e-03,  2.86398694e-01,\n",
       "        -1.63631348e-01,  4.39942994e-01, -5.90101609e-01],\n",
       "       [-3.05240383e-01, -7.93307379e-03,  2.86147107e-01,\n",
       "        -1.65255049e-01,  4.51877482e-01, -5.89241687e-01],\n",
       "       [-3.04347517e-01, -1.02582299e-02,  2.85499200e-01,\n",
       "        -1.66232056e-01,  4.57211838e-01, -5.88003917e-01],\n",
       "       [-3.03019610e-01, -1.12455583e-02,  2.85260086e-01,\n",
       "        -1.66156989e-01,  4.58548951e-01, -5.85340876e-01],\n",
       "       [-3.01461353e-01, -1.10699193e-02,  2.86335615e-01,\n",
       "        -1.65481342e-01,  4.58205428e-01, -5.81223692e-01],\n",
       "       [-2.99895948e-01, -1.04926128e-02,  2.88753486e-01,\n",
       "        -1.64843300e-01,  4.56848466e-01, -5.76971280e-01],\n",
       "       [-2.98339812e-01, -1.04075076e-02,  2.91497153e-01,\n",
       "        -1.64811099e-01,  4.56326756e-01, -5.74267485e-01],\n",
       "       [-2.96860492e-01, -1.15227017e-02,  2.93297752e-01,\n",
       "        -1.66196639e-01,  4.59064464e-01, -5.73895790e-01],\n",
       "       [-2.96049315e-01, -1.43820734e-02,  2.93456776e-01,\n",
       "        -1.69948871e-01,  4.63830889e-01, -5.75355049e-01],\n",
       "       [-2.96908330e-01, -1.92417586e-02,  2.92008608e-01,\n",
       "        -1.76303917e-01,  4.65756930e-01, -5.77250885e-01],\n",
       "       [-2.99989792e-01, -2.55340944e-02,  2.89369980e-01,\n",
       "        -1.84101980e-01,  4.62734294e-01, -5.77909298e-01],\n",
       "       [-3.04819133e-01, -3.18062507e-02,  2.85973994e-01,\n",
       "        -1.91386231e-01,  4.59462270e-01, -5.76268077e-01],\n",
       "       [-3.10351636e-01, -3.67086049e-02,  2.82123466e-01,\n",
       "        -1.96978572e-01,  4.62620864e-01, -5.72769816e-01],\n",
       "       [-3.15753121e-01, -3.99034069e-02,  2.78090806e-01,\n",
       "        -2.01129825e-01,  4.73697512e-01, -5.69096660e-01],\n",
       "       [-3.20572931e-01, -4.17657620e-02,  2.74297867e-01,\n",
       "        -2.04371339e-01,  4.88819822e-01, -5.66489742e-01],\n",
       "       [-3.24617571e-01, -4.26315247e-02,  2.71212303e-01,\n",
       "        -2.06420640e-01,  5.03844389e-01, -5.64457104e-01],\n",
       "       [-3.28073199e-01, -4.27828731e-02,  2.69019036e-01,\n",
       "        -2.06841288e-01,  5.16711372e-01, -5.61555993e-01],\n",
       "       [-3.31511964e-01, -4.27629965e-02,  2.67613950e-01,\n",
       "        -2.06113528e-01,  5.25396279e-01, -5.57166415e-01],\n",
       "       [-3.35368710e-01, -4.32332973e-02,  2.66919043e-01,\n",
       "        -2.05309750e-01,  5.26931671e-01, -5.51954217e-01],\n",
       "       [-3.39379096e-01, -4.46956316e-02,  2.66855704e-01,\n",
       "        -2.05257916e-01,  5.20679412e-01, -5.46928340e-01],\n",
       "       [-3.42684987e-01, -4.75723616e-02,  2.67024688e-01,\n",
       "        -2.06608222e-01,  5.11435452e-01, -5.42883100e-01],\n",
       "       [-3.44522428e-01, -5.22638601e-02,  2.66867908e-01,\n",
       "        -2.10043785e-01,  5.07357561e-01, -5.40589362e-01],\n",
       "       [-3.44791309e-01, -5.87513751e-02,  2.66302629e-01,\n",
       "        -2.15598344e-01,  5.14705104e-01, -5.40428253e-01],\n",
       "       [-3.44081851e-01, -6.61250470e-02,  2.65864540e-01,\n",
       "        -2.22076466e-01,  5.34397546e-01, -5.41573197e-01],\n",
       "       [-3.43359060e-01, -7.28260376e-02,  2.66021212e-01,\n",
       "        -2.27831602e-01,  5.62020244e-01, -5.42334120e-01],\n",
       "       [-3.43625253e-01, -7.76897640e-02,  2.66447491e-01,\n",
       "        -2.32213296e-01,  5.90823119e-01, -5.41681939e-01],\n",
       "       [-3.45502304e-01, -8.07788625e-02,  2.66132528e-01,\n",
       "        -2.36034432e-01,  6.16377350e-01, -5.40087450e-01],\n",
       "       [-3.48684049e-01, -8.31708755e-02,  2.64342500e-01,\n",
       "        -2.40657990e-01,  6.38906149e-01, -5.38830613e-01],\n",
       "       [-3.51839030e-01, -8.59720723e-02,  2.61474467e-01,\n",
       "        -2.46712391e-01,  6.61017220e-01, -5.38924243e-01],\n",
       "       [-3.53459668e-01, -8.94397556e-02,  2.58776409e-01,\n",
       "        -2.53533099e-01,  6.83719070e-01, -5.40722181e-01],\n",
       "       [-3.53030666e-01, -9.29823458e-02,  2.57133204e-01,\n",
       "        -2.59753628e-01,  7.04445701e-01, -5.44077213e-01],\n",
       "       [-3.51352662e-01, -9.61605449e-02,  2.56294881e-01,\n",
       "        -2.64646272e-01,  7.18068781e-01, -5.48609251e-01],\n",
       "       [-3.49702670e-01, -9.96642815e-02,  2.55395808e-01,\n",
       "        -2.68993272e-01,  7.20495046e-01, -5.53986854e-01],\n",
       "       [-3.48711296e-01, -1.04781930e-01,  2.53975135e-01,\n",
       "        -2.74331146e-01,  7.12542408e-01, -5.60002354e-01],\n",
       "       [-3.47976506e-01, -1.11597019e-01,  2.52278137e-01,\n",
       "        -2.81155478e-01,  7.00246045e-01, -5.66326097e-01],\n",
       "       [-3.46646004e-01, -1.18240906e-01,  2.50735934e-01,\n",
       "        -2.88233825e-01,  6.90983441e-01, -5.72500654e-01],\n",
       "       [-3.44279551e-01, -1.22479418e-01,  2.49458118e-01,\n",
       "        -2.93940135e-01,  6.90311606e-01, -5.78432879e-01],\n",
       "       [-3.41181239e-01, -1.23801626e-01,  2.48246263e-01,\n",
       "        -2.97942283e-01,  7.01109492e-01, -5.84568788e-01],\n",
       "       [-3.38100287e-01, -1.23549893e-01,  2.46977191e-01,\n",
       "        -3.01209465e-01,  7.21795433e-01, -5.91242322e-01],\n",
       "       [-3.35628427e-01, -1.23130169e-01,  2.45880139e-01,\n",
       "        -3.04463093e-01,  7.45982109e-01, -5.98047208e-01],\n",
       "       [-3.33609147e-01, -1.22547550e-01,  2.45393638e-01,\n",
       "        -3.07120939e-01,  7.67576562e-01, -6.04026799e-01],\n",
       "       [-3.31057029e-01, -1.20837449e-01,  2.45757593e-01,\n",
       "        -3.08096773e-01,  7.85640750e-01, -6.08345528e-01],\n",
       "       [-3.26946536e-01, -1.17552476e-01,  2.46743257e-01,\n",
       "        -3.07296763e-01,  8.02212189e-01, -6.10776012e-01],\n",
       "       [-3.21271653e-01, -1.13359660e-01,  2.47705912e-01,\n",
       "        -3.05747963e-01,  8.17276784e-01, -6.11737823e-01],\n",
       "       [-3.15210507e-01, -1.09268240e-01,  2.47915115e-01,\n",
       "        -3.04349438e-01,  8.27590548e-01, -6.12011221e-01],\n",
       "       [-3.10131742e-01, -1.05802590e-01,  2.47022466e-01,\n",
       "        -3.03198843e-01,  8.29027417e-01, -6.12413290e-01],\n",
       "       [-3.06518315e-01, -1.03073797e-01,  2.45289017e-01,\n",
       "        -3.02391185e-01,  8.20515312e-01, -6.13618707e-01],\n",
       "       [-3.03888686e-01, -1.01200749e-01,  2.43229579e-01,\n",
       "        -3.02786492e-01,  8.08108411e-01, -6.15976547e-01],\n",
       "       [-3.01545904e-01, -1.00279009e-01,  2.40968667e-01,\n",
       "        -3.05291692e-01,  8.03513557e-01, -6.19392973e-01],\n",
       "       [-2.99201789e-01, -1.00024965e-01,  2.38019757e-01,\n",
       "        -3.09570948e-01,  8.15034664e-01, -6.23558609e-01],\n",
       "       [-2.96916388e-01, -9.98270920e-02,  2.33775184e-01,\n",
       "        -3.14120289e-01,  8.40224076e-01, -6.28409841e-01],\n",
       "       [-2.94675261e-01, -9.93615591e-02,  2.28222643e-01,\n",
       "        -3.17755293e-01,  8.68901907e-01, -6.34159843e-01],\n",
       "       [-2.92244624e-01, -9.90171212e-02,  2.22140349e-01,\n",
       "        -3.20591620e-01,  8.91413585e-01, -6.40796117e-01],\n",
       "       [-2.89458305e-01, -9.95042057e-02,  2.16521263e-01,\n",
       "        -3.23501516e-01,  9.02221984e-01, -6.47893326e-01],\n",
       "       [-2.86518765e-01, -1.01012297e-01,  2.11778085e-01,\n",
       "        -3.27044321e-01,  8.99042878e-01, -6.55163482e-01],\n",
       "       [-2.83956776e-01, -1.02721236e-01,  2.07471678e-01,\n",
       "        -3.30946300e-01,  8.83388307e-01, -6.62739981e-01],\n",
       "       [-2.82289058e-01, -1.03114193e-01,  2.02792145e-01,\n",
       "        -3.34200471e-01,  8.61528921e-01, -6.70521439e-01],\n",
       "       [-2.81625241e-01, -1.01011327e-01,  1.97349077e-01,\n",
       "        -3.35742285e-01,  8.41272182e-01, -6.77785739e-01],\n",
       "       [-2.81491226e-01, -9.65063811e-02,  1.91583326e-01,\n",
       "        -3.35401411e-01,  8.26636635e-01, -6.83966614e-01],\n",
       "       [-2.81057634e-01, -9.07127645e-02,  1.86362365e-01,\n",
       "        -3.34025652e-01,  8.17123796e-01, -6.89211393e-01],\n",
       "       [-2.79719948e-01, -8.45509527e-02,  1.82066675e-01,\n",
       "        -3.32365130e-01,  8.12071483e-01, -6.93589792e-01],\n",
       "       [-2.77553201e-01, -7.81576110e-02,  1.78204040e-01,\n",
       "        -3.30195242e-01,  8.12952317e-01, -6.96373063e-01],\n",
       "       [-2.75068881e-01, -7.15032161e-02,  1.74135183e-01,\n",
       "        -3.26923268e-01,  8.19754957e-01, -6.96933907e-01],\n",
       "       [-2.72440846e-01, -6.49568737e-02,  1.70033206e-01,\n",
       "        -3.22595999e-01,  8.27350915e-01, -6.96099459e-01],\n",
       "       [-2.69177292e-01, -5.89848833e-02,  1.66760926e-01,\n",
       "        -3.17923827e-01,  8.28474236e-01, -6.96044045e-01],\n",
       "       [-2.64817264e-01, -5.37642710e-02,  1.64799191e-01,\n",
       "        -3.13709961e-01,  8.20084827e-01, -6.98755769e-01],\n",
       "       [-2.59803874e-01, -4.93062354e-02,  1.63654229e-01,\n",
       "        -3.10450058e-01,  8.05412047e-01, -7.04548055e-01],\n",
       "       [-2.55325110e-01, -4.56014746e-02,  1.62384187e-01,\n",
       "        -3.08139831e-01,  7.91025253e-01, -7.11849981e-01],\n",
       "       [-2.52245283e-01, -4.24238678e-02,  1.60397971e-01,\n",
       "        -3.06316118e-01,  7.82930344e-01, -7.18432853e-01],\n",
       "       [-2.50395647e-01, -3.91364732e-02,  1.57646601e-01,\n",
       "        -3.04429662e-01,  7.83510299e-01, -7.22834165e-01],\n",
       "       [-2.48928459e-01, -3.49794495e-02,  1.54334178e-01,\n",
       "        -3.02250986e-01,  7.90248223e-01, -7.24700295e-01],\n",
       "       [-2.47150573e-01, -2.97204143e-02,  1.50669811e-01,\n",
       "        -3.00035118e-01,  7.98552871e-01, -7.24489119e-01],\n",
       "       [-2.44923293e-01, -2.39119498e-02,  1.46872252e-01,\n",
       "        -2.98410679e-01,  8.07410075e-01, -7.23473428e-01],\n",
       "       [-2.42440512e-01, -1.83582057e-02,  1.43333281e-01,\n",
       "        -2.97948701e-01,  8.21171735e-01, -7.23636815e-01],\n",
       "       [-2.39910890e-01, -1.35363839e-02,  1.40626905e-01,\n",
       "        -2.98697679e-01,  8.44086970e-01, -7.26649519e-01],\n",
       "       [-2.37556664e-01, -9.81541574e-03,  1.39154604e-01,\n",
       "        -3.00333004e-01,  8.73754170e-01, -7.32509748e-01],\n",
       "       [-2.35662948e-01, -7.83422420e-03,  1.38778097e-01,\n",
       "        -3.02707809e-01,  9.01281749e-01, -7.39245396e-01],\n",
       "       [-2.34339679e-01, -7.85708626e-03,  1.38964355e-01,\n",
       "        -3.05734354e-01,  9.18330231e-01, -7.44210957e-01],\n",
       "       [-2.33299360e-01, -8.80697866e-03,  1.39294960e-01,\n",
       "        -3.08777060e-01,  9.23960603e-01, -7.46039093e-01],\n",
       "       [-2.32027812e-01, -8.76364528e-03,  1.39713040e-01,\n",
       "        -3.10863712e-01,  9.24856419e-01, -7.45535466e-01],\n",
       "       [-2.30121654e-01, -6.74589141e-03,  1.40357011e-01,\n",
       "        -3.11711522e-01,  9.29293659e-01, -7.44537340e-01],\n",
       "       [-2.27526578e-01, -3.58817548e-03,  1.41298871e-01,\n",
       "        -3.12092953e-01,  9.41538237e-01, -7.43832063e-01],\n",
       "       [-2.24703829e-01, -8.27199817e-04,  1.42370048e-01,\n",
       "        -3.12935296e-01,  9.62305958e-01, -7.42499415e-01],\n",
       "       [-2.22477505e-01,  9.69714482e-04,  1.43148547e-01,\n",
       "        -3.14216204e-01,  9.91582082e-01, -7.39617830e-01],\n",
       "       [-2.21324107e-01,  2.62387923e-03,  1.43225650e-01,\n",
       "        -3.14832505e-01,  1.02738055e+00, -7.36130873e-01],\n",
       "       [-2.20787693e-01,  5.24074707e-03,  1.42596508e-01,\n",
       "        -3.13690499e-01,  1.06293823e+00, -7.34115919e-01],\n",
       "       [-2.19911697e-01,  8.74956992e-03,  1.41752550e-01,\n",
       "        -3.11136609e-01,  1.08869102e+00, -7.34249690e-01],\n",
       "       [-2.18269068e-01,  1.16391671e-02,  1.41360622e-01,\n",
       "        -3.09174708e-01,  1.09758949e+00, -7.35026968e-01],\n",
       "       [-2.16281917e-01,  1.23207412e-02,  1.41880346e-01,\n",
       "        -3.09841029e-01,  1.08883914e+00, -7.34678751e-01],\n",
       "       [-2.14496009e-01,  1.06138050e-02,  1.43431587e-01,\n",
       "        -3.13383532e-01,  1.06885028e+00, -7.33021833e-01],\n",
       "       [-2.12819087e-01,  7.76284339e-03,  1.45887208e-01,\n",
       "        -3.18287705e-01,  1.04866594e+00, -7.31068845e-01],\n",
       "       [-2.10683434e-01,  5.31489877e-03,  1.49039989e-01,\n",
       "        -3.22804109e-01,  1.03668959e+00, -7.29450845e-01],\n",
       "       [-2.07885422e-01,  4.23514895e-03,  1.52695276e-01,\n",
       "        -3.25925238e-01,  1.03175510e+00, -7.27665330e-01],\n",
       "       [-2.05021675e-01,  4.68450060e-03,  1.56619441e-01,\n",
       "        -3.27229277e-01,  1.02475568e+00, -7.24727143e-01],\n",
       "       [-2.03014349e-01,  6.10352867e-03,  1.60406985e-01,\n",
       "        -3.26797353e-01,  1.00838207e+00, -7.20369587e-01],\n",
       "       [-2.02298600e-01,  7.58555895e-03,  1.63485434e-01,\n",
       "        -3.25501475e-01,  9.84460835e-01, -7.15456869e-01],\n",
       "       [-2.02472431e-01,  8.49488361e-03,  1.65392809e-01,\n",
       "        -3.24620480e-01,  9.61355853e-01, -7.11182416e-01],\n",
       "       [-2.02628170e-01,  8.75115451e-03,  1.66174402e-01,\n",
       "        -3.24775310e-01,  9.45891792e-01, -7.08283925e-01],\n",
       "       [-2.02040076e-01,  8.46775024e-03,  1.66484459e-01,\n",
       "        -3.25641642e-01,  9.38343950e-01, -7.07229796e-01],\n",
       "       [-2.00675785e-01,  7.60199136e-03,  1.67065229e-01,\n",
       "        -3.26742729e-01,  9.33745894e-01, -7.08449376e-01],\n",
       "       [-1.99134308e-01,  6.28749030e-03,  1.68032789e-01,\n",
       "        -3.27859473e-01,  9.28072458e-01, -7.11443219e-01],\n",
       "       [-1.98146532e-01,  5.23266398e-03,  1.68815630e-01,\n",
       "        -3.28646418e-01,  9.23059842e-01, -7.14079225e-01]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cam_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cb221916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RGB_img_emb_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [126]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight/img_human_model_100.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m----> 3\u001b[0m test_emb \u001b[38;5;241m=\u001b[39m \u001b[43mRGB_img_emb_tensor\u001b[49m[::\u001b[38;5;241m100\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m test_dic \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m:test_emb}\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(test_dic)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RGB_img_emb_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"weight/img_human_model_100.pth\", map_location=torch.device('cuda')))\n",
    "\n",
    "test_emb = RGB_img_emb_tensor[::100].to(\"cuda\")\n",
    "test_dic = {\"y\":test_emb}\n",
    "#print(test_dic)\n",
    "for i in range(10):\n",
    "    test_array = diffusion.p_sample_loop(\n",
    "                model,\n",
    "                (10, 138, 1,120),\n",
    "                clip_denoised=False,\n",
    "                progress=True,\n",
    "                model_kwargs=test_dic)\n",
    "    \n",
    "    \n",
    "    \n",
    "    np.save(\"Evaluation/our_cam_\"+str(i)+\".npy\",np.array(test_array.cpu()))\n",
    "    \n",
    "LS=[]\n",
    "for j in range(10):\n",
    "    ls=[]\n",
    "    for i in range(10):\n",
    "        ls.append(np.load(\"Evaluation/our_cam_\"+str(i)+\".npy\")[j].transpose(1,2,0))\n",
    "        \n",
    "    LS.append(np.array(ls).reshape(10,120,138))\n",
    "LS = np.array(LS)\n",
    "\n",
    "np.save(\"Evaluation/Divo_mmd.npy\",LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0649bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LS=[]\n",
    "for j in range(10):\n",
    "    ls=[]\n",
    "    for i in range(10):\n",
    "        ls.append(np.load(\"Evaluation/our_cam_\"+str(i)+\".npy\")[j].transpose(1,2,0))\n",
    "        \n",
    "    LS.append(np.array(ls).reshape(10,120,138))\n",
    "LS = np.array(LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02b72a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 120, 138)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "187bbe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Evaluation/Divo_mmd.npy\",LS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6021a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "Mean = np.load(\"single_mean.npy\")\n",
    "Std = np.load(\"single_std.npy\")\n",
    "test_data = test_array.cpu().detach()\n",
    "\n",
    "#generated_data = motion_reshape_tensor[21].permute(1,2,0)[0]\n",
    "generated_data = test_data[18].permute(1,2,0)[0]\n",
    "#generated_data = (generated_data*Std+Mean).reshape(-1,23,6)\n",
    "\n",
    "\n",
    "smoothed_generated_data = np.copy(generated_data)\n",
    "\n",
    "#print(smoothed_generated_data.shape)\n",
    "\n",
    "for axis in range(138):\n",
    "    smoothed_generated_data[:, axis] = gaussian_filter1d(generated_data[:, axis], sigma=1.5)\n",
    "\n",
    "\n",
    "generated_data = (smoothed_generated_data*Std+Mean).reshape(-1,23,6)\n",
    "\n",
    "\n",
    "\n",
    "D1 = np.array([1.0,0.0,0.0,\n",
    "               0.0,1.0,0.0,\n",
    "               0.0,0.0,1.0])\n",
    "motion1 = generated_data\n",
    "\n",
    "D2 = np.array([1.0,0.0,0.0,\n",
    "               0.0,1.0,0.0,\n",
    "               0.0,0.0,1.0])\n",
    "motion2 = generated_data\n",
    "\n",
    "\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p0.npy\",motion1)\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p1.npy\",motion2)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p0.npy\",D1)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p1.npy\",D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46b17eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6269e-03, -7.0824e-03, -4.8302e-02,  3.4259e-02, -1.2544e-02,\n",
       "          -7.8722e-03, -9.1139e-03,  6.8659e-03, -2.0102e-02, -2.9830e-02,\n",
       "          -3.3396e-03, -1.3117e-02, -3.3835e-02,  5.2381e-04, -5.8988e-03,\n",
       "          -1.1672e-02,  1.5552e-02,  9.4287e-03,  3.7366e-03,  1.0995e-02,\n",
       "          -3.6067e-03,  1.6236e-02,  3.2856e-03, -1.1692e-02, -1.9179e-02,\n",
       "           1.5309e-02,  4.3737e-03,  5.0000e-03, -1.4255e-02, -1.9424e-02,\n",
       "           2.5795e-02, -1.4747e-02, -1.5030e-02, -2.4430e-02, -1.0824e-02,\n",
       "          -1.5499e-02, -1.2518e-03,  6.2621e-03,  8.1197e-03, -4.0269e-02,\n",
       "          -7.9910e-03, -1.8712e-02, -9.6054e-03,  5.1850e-03, -1.3816e-02,\n",
       "          -8.0370e-03, -6.9571e-03,  1.8986e-02, -1.8182e-02, -8.6861e-03,\n",
       "          -8.7420e-03,  1.1795e-03,  4.8186e-03, -2.1707e-02, -1.8685e-03,\n",
       "           6.6663e-03, -5.0312e-03, -9.2422e-03,  1.3447e-02,  2.2549e-02,\n",
       "           6.9266e-03, -1.1157e-02, -1.8490e-02,  1.6490e-04, -1.2572e-03,\n",
       "          -9.9702e-03,  1.5047e-02,  1.2893e-02,  2.4468e-03,  6.7621e-03,\n",
       "           2.4189e-03,  8.8571e-03,  9.5987e-03, -5.7759e-03, -1.0846e-02,\n",
       "          -1.1489e-02,  1.7676e-02, -7.3121e-03,  5.7949e-03, -3.6666e-03,\n",
       "          -2.9669e-02, -1.7089e-02, -2.4859e-03, -9.7223e-03,  1.0127e-02,\n",
       "           9.8594e-04, -9.4102e-04,  1.8501e-02,  1.5940e-02,  1.4019e-02,\n",
       "          -6.2677e-03, -9.0532e-03,  1.0635e-02, -1.0610e-02,  7.4534e-04,\n",
       "           2.4953e-03,  1.0133e-02,  1.4044e-02, -1.1319e-02, -2.0393e-02,\n",
       "          -2.4108e-03, -1.0069e-02, -1.1959e-03, -1.6869e-02, -7.7700e-03,\n",
       "           2.9486e-02, -1.5336e-02, -1.8767e-02,  2.7579e-04,  2.3984e-02,\n",
       "           7.7106e-03, -1.1444e-02, -9.1436e-03,  1.0176e-02, -6.1308e-05,\n",
       "          -1.5170e-02,  6.9611e-03,  2.7880e-02, -1.5552e-02,  3.9544e-03]],\n",
       "\n",
       "        [[ 1.9763e-02, -1.7869e-02, -2.0028e-02, -1.1110e-03, -1.2001e-02,\n",
       "           8.6756e-03, -1.4266e-02,  6.8104e-03,  3.0555e-02, -3.9437e-02,\n",
       "           5.1931e-03, -1.4315e-02,  2.5287e-02,  4.3840e-03,  1.3427e-02,\n",
       "          -2.2154e-02, -3.1854e-02,  2.7321e-02,  9.8526e-03,  2.0323e-02,\n",
       "           4.0009e-03, -4.0617e-02,  5.6107e-02,  1.5037e-02, -1.2291e-02,\n",
       "          -4.4100e-02, -7.4901e-03, -1.3977e-02,  1.6172e-02, -1.6576e-02,\n",
       "           4.0372e-02, -1.3168e-02, -2.7903e-02,  2.3491e-03,  2.3962e-02,\n",
       "          -1.2394e-04,  1.8490e-02, -1.4012e-03,  2.6701e-03,  9.1811e-03,\n",
       "           1.3512e-02, -2.3310e-02,  7.8279e-03,  3.4789e-04,  9.8905e-03,\n",
       "          -2.4971e-02, -7.0226e-03, -9.0514e-04,  1.7268e-03, -4.1882e-03,\n",
       "           4.2275e-03, -1.8231e-02,  1.5089e-03, -1.4960e-02, -4.9554e-03,\n",
       "          -1.1708e-02,  4.0205e-03, -5.8102e-03, -8.3382e-03, -1.0622e-02,\n",
       "           5.9726e-03, -1.3324e-02, -7.6427e-03, -1.1287e-02, -8.6069e-04,\n",
       "           7.6502e-03,  1.7843e-02, -1.5337e-02, -2.7046e-02, -7.1595e-03,\n",
       "          -2.3435e-02,  1.4692e-03, -1.0731e-02, -2.4365e-03, -4.2758e-03,\n",
       "           1.0265e-02, -1.6098e-05, -9.2542e-03, -3.0594e-02, -3.1617e-03,\n",
       "          -4.1948e-03, -5.2002e-03,  5.6898e-03,  2.5221e-02,  4.3563e-04,\n",
       "           9.9966e-03,  1.9219e-02,  1.3549e-02,  5.4606e-03, -1.8100e-02,\n",
       "           8.6294e-03, -2.3420e-02, -9.5012e-03, -2.0621e-03, -2.3432e-02,\n",
       "          -1.1244e-02,  8.8640e-04,  8.9172e-03, -1.6085e-02, -1.2893e-03,\n",
       "           1.4880e-02, -1.0067e-02, -1.0327e-02,  1.4288e-02, -1.6650e-02,\n",
       "           2.0851e-02, -8.5736e-03,  1.2215e-02,  1.5175e-02, -6.2241e-03,\n",
       "           6.6002e-03, -2.2784e-03, -1.7999e-02,  4.3215e-03,  5.1202e-03,\n",
       "           2.5169e-02, -3.4652e-03, -8.4260e-03, -1.6281e-02, -3.7573e-03]],\n",
       "\n",
       "        [[ 2.0231e-02, -2.0360e-02, -5.1452e-02, -1.1875e-03,  3.8172e-03,\n",
       "          -2.2683e-02, -2.3531e-02, -2.4873e-02, -1.7476e-02,  1.7486e-02,\n",
       "           1.8304e-02, -1.8282e-02, -1.8027e-02, -1.2739e-02, -1.6735e-03,\n",
       "           1.2360e-03,  1.1574e-02, -2.2886e-03, -3.1438e-02, -2.9541e-03,\n",
       "          -1.0024e-02,  3.9334e-03, -9.2937e-03, -6.1474e-03,  1.7625e-02,\n",
       "           5.0210e-03, -1.1411e-02,  5.8292e-03,  2.8824e-02,  1.2922e-02,\n",
       "           3.4031e-03,  1.9829e-02,  6.5210e-04,  6.3958e-03, -4.8174e-03,\n",
       "           3.3147e-03,  1.5707e-03, -4.5771e-04, -1.9562e-02, -4.2921e-03,\n",
       "           1.0186e-02,  1.9015e-02, -9.9266e-03, -1.0931e-04,  2.3750e-03,\n",
       "          -3.6763e-02,  5.9160e-03, -1.1473e-02, -3.6838e-04, -2.9594e-02,\n",
       "          -7.6061e-03, -8.0024e-03,  2.0326e-03, -1.2808e-02,  1.1823e-03,\n",
       "          -1.7012e-02,  4.7746e-03, -6.4549e-03, -9.9915e-03, -4.1794e-03,\n",
       "          -6.7633e-03, -5.4186e-03, -2.1532e-02,  1.9221e-02, -7.7069e-03,\n",
       "          -3.5905e-03,  1.2047e-03,  1.4426e-02,  4.6123e-03,  1.5076e-03,\n",
       "          -1.2699e-03, -1.7985e-02, -5.5546e-03, -2.6921e-02, -1.4698e-02,\n",
       "          -1.0262e-02, -6.8854e-03,  3.5930e-03,  6.8461e-03,  5.4026e-03,\n",
       "          -1.8980e-02, -1.2558e-02, -1.0950e-02,  7.1741e-03,  8.7667e-03,\n",
       "          -3.1789e-03, -1.7543e-02, -7.8777e-03,  2.7096e-04,  6.6376e-03,\n",
       "          -1.6017e-03,  5.0010e-03, -9.2563e-03, -5.2207e-03, -2.0694e-03,\n",
       "           5.0941e-03, -1.7054e-02,  7.6090e-03, -1.5238e-02, -1.5411e-02,\n",
       "          -1.9342e-02, -1.7433e-02,  1.0821e-02,  1.0363e-02,  1.1079e-02,\n",
       "          -2.0273e-02,  2.4326e-02,  6.2779e-03,  5.2842e-03,  2.8846e-03,\n",
       "          -6.7506e-03, -3.7917e-02,  1.1159e-03,  1.0451e-02,  2.1126e-05,\n",
       "           6.6396e-03,  8.2339e-03,  2.9671e-03,  8.4879e-03, -1.2799e-02]],\n",
       "\n",
       "        [[ 2.8087e-02,  7.2888e-02, -1.6471e-03,  2.4289e-02,  1.8813e-02,\n",
       "           2.3879e-02, -2.5844e-02, -2.0633e-02, -1.8741e-03,  4.0367e-02,\n",
       "          -2.5272e-02,  2.1967e-03,  1.8150e-04,  2.2752e-03, -1.4593e-02,\n",
       "          -8.9781e-03, -1.7962e-02,  2.6608e-03, -3.1393e-02, -1.6231e-02,\n",
       "           1.3411e-02,  1.1837e-02,  5.8729e-04, -1.0214e-02, -1.6142e-02,\n",
       "          -2.1539e-03, -5.7377e-03, -2.9392e-03, -7.3396e-03, -1.1557e-03,\n",
       "           8.5526e-03, -6.7107e-03, -1.8074e-02,  3.3671e-03, -2.6389e-02,\n",
       "          -1.5157e-02, -3.8419e-03,  1.4997e-02,  2.7149e-02,  1.0498e-03,\n",
       "           9.6011e-03, -4.2601e-03,  3.3074e-03,  9.6729e-03,  2.4250e-02,\n",
       "          -1.3228e-02, -1.7115e-03,  5.0473e-03,  3.4971e-02, -2.5929e-03,\n",
       "           8.8303e-03,  1.1190e-02, -3.5284e-03,  4.8565e-03, -1.2467e-02,\n",
       "          -1.4884e-02,  6.4694e-03,  1.1016e-02,  5.3879e-03,  8.0193e-03,\n",
       "          -1.3786e-02, -1.1360e-02,  7.7028e-04,  9.6546e-03, -1.5143e-02,\n",
       "           1.4200e-02, -8.8098e-04, -9.6511e-03,  1.4204e-02,  2.2891e-02,\n",
       "           1.5817e-02, -4.7761e-03,  2.8029e-02, -1.2844e-02, -7.9252e-03,\n",
       "           1.2546e-02,  1.2610e-02, -1.6950e-02,  1.0847e-02, -3.0096e-03,\n",
       "          -2.5921e-02, -3.3673e-03,  1.7170e-02,  1.2566e-02,  1.1836e-02,\n",
       "           2.6338e-03, -5.7756e-03,  2.2591e-02, -3.4214e-03, -1.0976e-02,\n",
       "           7.2144e-04,  1.1359e-02,  5.9306e-03, -1.2463e-03,  5.3107e-03,\n",
       "           3.1199e-02, -4.6859e-03,  1.6224e-02, -3.2035e-02,  1.8454e-02,\n",
       "           2.0256e-02,  1.5084e-02,  2.1399e-02, -2.3968e-03, -1.8590e-02,\n",
       "           3.1081e-02, -7.8372e-03, -1.4197e-04, -1.1369e-02, -2.4270e-02,\n",
       "          -3.2932e-03, -2.1722e-02,  1.8797e-02, -1.3083e-02,  1.6673e-02,\n",
       "          -1.6342e-02,  1.1698e-02,  2.1421e-02, -8.8647e-03, -2.2970e-03]],\n",
       "\n",
       "        [[-7.6834e-03, -2.0989e-02,  2.5749e-02,  2.4260e-02, -2.4444e-02,\n",
       "           2.0947e-02,  1.8275e-02, -1.9760e-02, -3.8064e-02,  8.5227e-03,\n",
       "           1.8791e-02,  3.2382e-02,  1.6537e-02,  1.2267e-02, -1.5830e-03,\n",
       "           6.2182e-03, -3.4687e-03, -3.2478e-02,  2.9443e-03,  2.6136e-02,\n",
       "          -1.3408e-04,  6.1166e-03,  1.2189e-02,  1.0929e-03,  1.6387e-02,\n",
       "          -2.2060e-02,  6.2679e-03, -1.4249e-02, -7.1666e-03,  1.2927e-02,\n",
       "          -1.0527e-03,  2.2531e-03,  2.8010e-03, -2.7119e-03,  5.5930e-03,\n",
       "          -1.5694e-02, -4.5318e-03, -2.3018e-03,  1.1138e-02,  7.8815e-03,\n",
       "          -3.3434e-03,  5.0328e-02, -2.2609e-03, -2.4290e-02,  3.1393e-02,\n",
       "           1.4354e-02, -1.7240e-02,  1.3532e-02, -1.0188e-02, -5.7391e-03,\n",
       "          -1.4773e-02, -1.2350e-03,  2.9303e-03,  1.8840e-02,  5.4520e-03,\n",
       "          -3.0011e-03,  1.3756e-02, -1.0678e-02, -1.1971e-02, -2.6025e-03,\n",
       "          -1.5823e-02,  2.2182e-02,  2.7201e-03, -2.9884e-03, -1.7898e-02,\n",
       "           1.9619e-02, -4.6353e-03, -5.8476e-03,  1.3670e-02, -8.0434e-03,\n",
       "          -7.5595e-04,  1.5080e-02, -1.5597e-02,  1.3892e-02, -9.0572e-03,\n",
       "           1.1983e-02, -2.2285e-03, -3.4162e-02, -1.0333e-02, -1.7763e-02,\n",
       "          -3.0771e-03,  2.1806e-02,  5.1506e-03,  1.6890e-02, -6.2649e-03,\n",
       "           7.7049e-03,  1.2860e-02, -2.6891e-04,  7.4067e-03,  6.2754e-03,\n",
       "           3.0113e-02,  3.7134e-04,  2.1209e-03,  7.7063e-03, -1.9457e-03,\n",
       "           4.3617e-03,  1.1036e-02, -2.2050e-02, -8.4879e-03, -8.2474e-03,\n",
       "           2.3671e-02,  2.1839e-03,  3.0967e-04,  1.2895e-02, -9.3598e-03,\n",
       "           3.4468e-02,  1.5049e-02,  1.5953e-02,  2.6162e-02,  3.1148e-02,\n",
       "           1.3987e-02,  2.1020e-03,  7.4597e-03, -6.5459e-03, -1.2279e-02,\n",
       "          -2.3952e-02,  3.2388e-03,  1.8256e-03,  2.0228e-03, -4.8286e-03]],\n",
       "\n",
       "        [[ 2.8224e-02,  3.0234e-02, -3.5532e-02, -2.1979e-02,  1.1917e-02,\n",
       "           1.8733e-02, -4.4481e-02,  2.7786e-02, -2.3174e-02, -5.6067e-02,\n",
       "          -1.8329e-03,  1.4375e-03, -7.2255e-03,  1.7443e-03, -5.2148e-04,\n",
       "           2.9089e-04, -3.0422e-02,  7.9934e-03, -4.1107e-03, -1.9814e-02,\n",
       "          -3.7653e-02, -1.7931e-02,  4.1860e-02, -7.9215e-03,  1.3209e-02,\n",
       "          -1.0323e-02, -1.8693e-02,  3.0226e-02,  1.0803e-02, -2.1398e-02,\n",
       "          -3.1154e-04, -3.2828e-03, -3.8143e-02,  2.4171e-02, -2.5905e-02,\n",
       "           2.3690e-02,  1.3935e-02, -1.3139e-02,  3.3406e-03,  2.6176e-04,\n",
       "           3.3996e-03, -2.2804e-03,  8.2400e-03, -9.3776e-03, -2.4289e-02,\n",
       "          -4.6137e-03, -4.2746e-03,  7.2880e-03,  2.5256e-02, -5.6492e-03,\n",
       "          -3.2775e-02,  2.6798e-03, -5.1215e-03, -6.0067e-03, -7.9165e-03,\n",
       "          -3.2686e-04, -8.5785e-03, -5.9350e-03,  3.4634e-03, -1.0013e-02,\n",
       "          -2.6083e-03, -1.7285e-03,  1.5224e-02,  6.1559e-03,  1.1566e-02,\n",
       "          -2.4809e-02,  7.1066e-03, -3.4919e-03, -2.1391e-02, -1.5992e-02,\n",
       "          -5.9723e-03, -1.1989e-03, -3.3841e-02,  1.0530e-02,  1.7928e-03,\n",
       "          -2.0997e-03, -2.1959e-02,  2.3074e-04, -9.2623e-03, -1.5750e-02,\n",
       "          -2.7300e-02, -2.9443e-02, -1.8262e-03, -3.1265e-03,  8.8290e-03,\n",
       "           1.8455e-02, -1.5816e-02, -2.0544e-02, -3.1197e-02, -1.4739e-02,\n",
       "           1.0688e-02, -2.6084e-02,  7.0657e-03,  7.6528e-03,  3.1330e-03,\n",
       "          -2.0814e-02,  1.1808e-02, -2.6848e-02,  1.2684e-02, -1.1018e-02,\n",
       "          -2.8685e-02,  8.3000e-03, -1.5175e-02,  3.3831e-02,  2.6204e-03,\n",
       "          -1.8715e-02, -1.3988e-02,  8.5547e-04,  1.0116e-02, -3.2616e-02,\n",
       "          -1.4775e-02,  4.7055e-03, -8.9047e-03, -1.0133e-02, -3.5761e-03,\n",
       "          -1.0314e-02, -1.1836e-02,  1.4279e-02, -1.6796e-02, -1.3443e-02]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:,:6][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4b9a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/home/home/BoyuanCheng/priorMD/../Camera/priorMD/vis_generated_data.py\", line 11, in <module>\r\n",
      "    from utils.humanml3d import Convert_Pose_to_Joints3D\r\n",
      "  File \"/home/home/BoyuanCheng/Camera/priorMD/utils/humanml3d.py\", line 67, in <module>\r\n",
      "    smpl_skeleton = np.load(smpl_skeketon_path)\r\n",
      "  File \"/home/home/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages/numpy/lib/npyio.py\", line 417, in load\r\n",
      "    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\r\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'priorMD/dataset/Self_HumanML3D/smpl_static_skeleton.npy'\r\n"
     ]
    }
   ],
   "source": [
    "!python ../Camera/priorMD/vis_generated_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb57941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
